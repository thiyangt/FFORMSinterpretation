---
title: "Peeking inside FFORMS: Feature-based FORecast Model Selection"
author:
  - familyname: Talagala
    othernames: Thiyanga S
    address: Department of Econometrics and Business Statistics, \newline Monash University, VIC 3800, Australia.
    email : thiyanga.talagala@monash.edu
    correspondingauthor: true
  - familyname: Hyndman
    othernames: Rob J
    address: Department of Econometrics and Business Statistics, \newline Monash University, VIC 3800, Australia.
    email : rob.hyndman@monash.edu
  - familyname: Athanasopoulos
    othernames: George
    address: Department of Econometrics and Business Statistics, \newline Monash University, VIC 3145, Australia.
    email : george.athanasopoulos@monash.edu
abstract: "Features of time series are useful in identifying suitable models for forecasting. Talagala, Hyndman & Athanasopoulos (2018) proposed a classification framework, labelled FFORMS (Feature-based FORecast Model Selection), which selects forecast models based on features calculated from the time series. The FFORMS framework builds a mapping that relates the features of a time series to the “best” forecast model using the random forest algorithm. In this paper we explore what is happening under the hood of the FFORMS framework. This is accomplished using model-agnostic machine learning interpretability approaches. The analysis provides a valuable insight into how different features and their interactions affect the choice of forecast model selection. This gives a more refined picture of the relationship between features and the choice of forecast model which is particularly valuable for ongoing research in the field of feature-based time series analysis."
keywords: "forecasting, time series, machine learning interpretability, black-box models, LIME"
wpnumber: no/yr
jelcodes: C10,C14,C22
blind: true
cover: true
toc: false
bibliography: references.bib
biblio-style: authoryear-comp
output:
  MonashEBSTemplates::workingpaper:
    fig_caption: yes
    fig_height: 5
    fig_width: 8
    includes:
      in_header: preamble.tex
    keep_tex: yes
    number_sections: yes
    citation_package: biblatex
---

```{r setup, include=FALSE, cache=FALSE}
library(knitr)
knitr::opts_chunk$set(echo = FALSE, cache=TRUE, messages=FALSE, warning=FALSE, fig.path = 'figures/', dev=c('png'), fig.pos= "h", external = TRUE)
# Make sure you have the latest version of rmarkdown and bookdown
#devtools::install_github("rstudio/rmarkdown")
#devtools::install_github("rstudio/bookdown")
read_chunk("src/main.R") # analysis of yearly, quarterly and monthly
read_chunk("src/main_weekly.R")
read_chunk("src/main_daily.R")
read_chunk("src/main_hourly.R")
read_chunk("src/main_lime.R")
read_chunk("src/twowayPDPplots.R")
read_chunk("src/main_daily_hourly.R")
library(Mcomp)
library(ggplot2)
library(patchwork)
library(tidyverse)
library(reshape2)
library(ggplot2)
library(grid)
library(gridExtra)
library(ggrepel)
library(png)
library(tsfeatures)
library(tidyverse)
library(ggpubr)
library(forestviews)
#install_github(repo = 'thiyangt/forestviews')
library(networkD3)
library(RColorBrewer)
library(iml) #machine learning interpretability package
library(ggcorrplot) # to draw  ggcorrplot
library(ggpubr)
library(lime)
library(seer)
```


# Introduction{#intro}

The field of time series forecasting field has been evolving for a few decades now and has introduced a wide variety of models for forecasting. However, for a given time series the selection of an appropriate forecast-model among many possibilities is not straight forward. This selection is one of the most difficult tasks as each method perform best for some but not all tasks. The features of a time series are considered to be an important factor in identifying suitable forecasting models [@collopy1992rule; @meade2000evidence; @makridakis2000m3; @wang2009rule]. However, a comprehensive description of the relationship between the features and the performance of algorithms is rarely discussed. 

There have been several recent studies on the use of meta-learning approaches to automate the forecast-model selection based on the features computed from the time series [@shah1997model; @prudencio2004meta; @lemke2010meta; @kuck2016meta]. Meta-learning approach provides a systematic guidance on model selection based on knowledge acquired from historical data sets. The key idea is, forecast-model selection is posed as a supervised learning task. Each time series in the meta-data set is represented as a vector of features and labelled according to the "best" forecast-model (i.e. for example model with lowest MASE over a test set, etc.). Then a meta-learner is trained to identify a suitable forecast-model (usually a machine learning algorithm is used). In the era of big data, such an automated model selection process is necessary because the cost of invoking all possible forecast-models is prohibitive. However, the existing literature suffers from the limitation of providing answers to questions such as: i) How features are related to the property being modelled?; ii) How features interact with each other to identify a suitable forecast-model?; iii) Which features contribute most to the classification process?, etc. Addressing such questions can enhance the understanding of the relations between features and model selection outcomes. To the best of our knowledge, a very limited effort has been taken to understand how the meta-learners are making its decisions and what is really happening inside these complex model structures. Providing transparency will result, building trust in the prediction results of the meta-learner.

Furthermore, besides from the goal of developing an automated forecast-model selection framework very few researchers have made an attempt to provide a description of the relationship between the features and the choice of different forecast-models [@schnaars1984situational;  @wang2009rule;@lemke2010meta; @petropoulos2014horses]. These studies are limited by the scale of problem instances used, the diversity of forecast-models implemented, and the limited number of features considered to identify the relationship between features and forecast-model performance. 

To fill this gap, this paper makes a first step towards providing a comprehensive analysis of the relationship between time series features and forecast-model selection using machine learning interpretability techniques. This paper builds on the method from our previous work @fforms,  in which we introduced the FFORMS (Feature-based FORecast Model Selection) framework. A random forest is used to model the relationship between features and "best" performing forecast-model. A large collection of time series is used to train the meta-learner.

In this article, we make the following contributions:

1. We extend the FFORMS framework to handle weekly, daily and hourly series. We also extend the diversity of forecast-models used as class labels. The contribution of this paper differs from previously published work related to meta-learning [@prudencio2004meta; @lemke2010meta; @kuck2016meta] in three ways : i) a more extensive collection of features is used (35 different feature types are used which are simple and easy to compute), ii) the diversity of forecast-models considered as class labels, and iii) capability of handling high frequency data;
1. We analyse the application of the FFORMS framework to the M4-competition data. We generated point forecasts and prediction intervals for the M4-competition time series data, and is shown to yield accurate forecast comparable to several benchmarks and other commonly used automated approaches of time series forecasting. Our approach achieved a high accuracy rate based on individual forecast-model selection rule. 
1. The main contribution of the paper is to explore the relationship between features of time series and the choice of forecast-model selection using the FFORMS framework. We explore the role of features in two different perspectives: i) Global perspective of feature contribution: the overall role of features in the choice of different forecast-models and ii) Local perspective of feature contribution: we zoom into local regions of the data to identify which features contribute most to classify a specific instance. 

The remainder of the paper is structured as follows. In \autoref{fforms} we describe the application of FFORMS framework to M4-competition data. \autoref{machinelearning} gives background on machine learning interpretability techniques that are used to identify role of features in forecast-model selection. In \autoref{results} we discuss the results. \autoref{conclusions} concludes.

# FFORMS application to M4 competition data{#fforms}

The FFORMS framework consists of two main components: i) *offline phase*, which includes the development of a meta-learner and ii) *online phase*, using the meta-learner identify the "best" forecast-model. We develop separate classifiers for yearly, monthly, quarterly, weekly, daily and hourly series. 


## FFORMS framework: offline phase

### Reference set

We call the collection of time series used for training the meta-leaner as the "reference set". The reference set consist of two set of time series: i) observed sample and ii) simulated time series. 

### Observed sample

We use the time series from the M1, M3 and M4 competitions as the observed sample.  Note that from the M4 competition randomly selected subset of time series are used for the observed sample. The rest is used as a validation set to evaluate the meta-learner. Table \ref{observedsample} summarizes the number of time series in the observed sample.

\begin{table}[!h]
\centering
\caption{Composition of the time series in the observed sample}
\label{observedsample}
\begin{tabular}{l|rrr}
\multirow{2}{*}{Frequency} & \multicolumn{3}{l}{Observed Sample} \\
                  &   M1    &    M3   &    M4  \\ \hline
  Yearly          &   181    &   645    &   22000  \\
  Quarterly       &   203    &    756   &   23000\\
  Monthly         &   617    &    1428   &  47000\\
  Weekly          &   -    &   -    &   259\\
  Daily           &   -    &   -    &   4001\\
  Hourly          &   -    &    -   &  350\\ \hline
\end{tabular}
\end{table}

### Simulated time series

As described in @fforms, we augment the reference set by adding multiple time series simulated based on each series in the M4 competition. We use several automated algorithms to simulate multiple time series. Table \ref{simulation} shows the data generating algorithms used for each frequency. The `ets()` and `auto.arima()` functions available in the forecast package in R [@forecast] are used to simulate yearly, quarterly and monthly data from exponential smoothing and ARIMA models. The `stlf()` function also available in the forecast package is used to simulate multiple time series based on multiple seasonal decomposition approach. Using the above functions, we fit models to each time series in the M4 database and then simulate multiple time series from the selected models. In this experiment the length of the simulated time series is set to be equal to: length of the training
period specified in the M4 competition + length of the forecast horizon specified in the competition. For example, the series with id "Y13190" contains a training period of length 835. The length of the simulated series generated based on this series is equals to 841 (835+6). Before simulating time series from daily and hourly series, we convert the time series into multiple seasonal time series (msts) objects. For daily time series with length less than 366, the frequency set to 7 and longer time series are converted to multiple seasonal time series objects with frequencies set to 7 and 365.25. For hourly series, we set the frequencies to 24 and 168 to handle multiple frequencies corresponds to time-of-day pattern and time-of-week pattern respectively. 

\begin{table}[!h]
\centering
\caption{Automatic forecasting algorithms used to simulate time series}
\label{simulation}
\begin{tabular}{lllllll}
 Algorithm & Y & Q & M & W & D &  H \\ \hline
 ets() & \checkmark & \checkmark & \checkmark &  &  &  \\
auto.arima() & \checkmark & \checkmark & \checkmark &  &  &  \\
stlf() &  &  &  & \checkmark & \checkmark & \checkmark\\ \hline
\end{tabular}
\end{table}

We should re-emphasize that all the observed time series and the simulated time series form the reference set to build our meta-learner. Once we create the reference set for random forest training we split each time series in the reference set into training period and test period. 

### Input: features

The FFORMS framework operates on features calculated from the time series. For each time series in the reference set, features are calculated based on the training period of the time series. 

\begin{table}[!htp]
\centering\footnotesize\tabcolsep=0.12cm
\caption{Time series features}
\label{feature}
\begin{tabular}{llp{8,8cm}cccc}
\toprule
\multicolumn{2}{c}{Feature} & Description & Y & Q/M & W & D/H\\
\midrule
1  & T              & length of time series                                                                   & \yes  & \yes & \yes & \yes\\
2  & trend          & strength of trend                                                                       & \yes  & \yes & \yes & \yes\\
3  & seasonality 1    & strength of seasonality corresponds to frequency 1                                                              & -     & \yes & \yes & \yes\\
4  & seasonality 2    & strength of seasonality corresponds to frequency 2                                                              & -     & - & -& \yes\\
5  & linearity      & linearity                                                                               & \yes  & \yes & \yes & \yes\\
6  & curvature      & curvature                                                                               & \yes  & \yes & \yes & \yes\\
7  & spikiness      & spikiness                                                                               & \yes  & \yes & \yes & \yes\\
8  & e\_acf1        & first ACF value of remainder series                                                     & \yes  & \yes & \yes & \yes\\
9  & stability      & stability                                                                               & \yes  & \yes & \yes & \yes\\
10  & lumpiness      & lumpiness                                                                               & \yes  & \yes & \yes & \yes\\
11 & entropy        & spectral entropy                                                                        & \yes  & \yes & \yes & \yes\\
12 & hurst          & Hurst exponent                                                                          & \yes  & \yes & \yes & \yes\\
13 & nonlinearity   & nonlinearity                                                                            & \yes\ & \yes & \yes & \yes\\
14 & alpha          & ETS(A,A,N) $\hat\alpha$                                                                 & \yes  & \yes & \yes & -\\
15 & beta           & ETS(A,A,N) $\hat\beta$                                                                  & \yes  & \yes & \yes & - \\
16 & hwalpha        & ETS(A,A,A) $\hat\alpha$                                                                 & -     & \yes & - & -\\
17 & hwbeta         & ETS(A,A,A) $\hat\beta$                                                                  & -     & \yes & - & - \\
18 & hwgamma        & ETS(A,A,A) $\hat\gamma$                                                                 & -     & \yes & - &-\\
19 & ur\_pp         & test statistic based on Phillips-Perron test                                            & \yes  & - & - & - \\
20 & ur\_kpss       & test statistic based on KPSS test                                                       & \yes  & - & - & - \\
21 & y\_acf1        & first ACF value of the original series                                                  & \yes  & \yes & \yes & \yes\\
22 & diff1y\_acf1   & first ACF value of the differenced series                                               & \yes  & \yes & \yes & \yes\\
23 & diff2y\_acf1   & first ACF value of the twice-differenced series                                         & \yes  & \yes & \yes & \yes\\
24 & y\_acf5        & sum of squares of first 5 ACF values of original series                                 & \yes  & \yes & \yes & \yes\\
25 & diff1y\_acf5   & sum of squares of first 5 ACF values of differenced series                              & \yes  & \yes & \yes & \yes\\
26 & diff2y\_acf5   & sum of squares of first 5 ACF values of twice-differenced series                        & \yes  & \yes & \yes & \yes \\
27 & seas\_acf1     & autocorrelation coefficient at first seasonal lag                                       & -     & \yes & \yes & \yes\\
28 & sediff\_acf1   & first ACF value of seasonally-differenced series                                        & -     & \yes & \yes & \yes\\
29 & sediff\_seacf1 & ACF value at the first seasonal lag of seasonally-differenced series                    & -     & \yes & \yes & \yes\\
30 & sediff\_acf5   & sum of squares of first 5 autocorrelation coefficients of seasonally-differenced series & -     & \yes & \yes & \yes\\
31 & seas\_pacf     & partial autocorrelation coefficient at first seasonal lag & -     & \yes & \yes & \yes\\
32 & lmres\_acf1    & first ACF value of residual series of linear trend model                                & \yes  & - & - & -\\
33 & y\_pacf5       & sum of squares of first 5 PACF values of original series                                & \yes  & \yes & \yes & \yes\\
34 & diff1y\_pacf5  & sum of squares of first 5 PACF values of differenced series                             & \yes  & \yes & \yes & \yes\\
35 & diff2y\_pacf5  & sum of squares of first 5 PACF values of twice-differenced series                       & \yes  & \yes & \yes & \yes\\
\bottomrule
 \end{tabular}
\end{table}

The description of the features calculated under each frequency category is shown in Table \ref{feature}. A comprehensive description of the features used in the experiment is given in @fforms.

### Output: class-labels

The description of class labels considered under each frequency is shown in Table \ref{classlabels}. Note that these added to @fforms. Most of the labels given in Table \ref{classlabels} are self-explanatory labels. In STL-AR, mstlets, and mstlarima, first STL decomposition method applied to the time series and then seasonal naive method is used to forecast the seasonal component. Finally, AR, ETS and ARIMA models are used to forecast seasonally adjusted data respectively. We fit the corresponding models outlined in Table \ref{classlabels} to each series in the reference set. The models are estimated using the training period for each series, and forecasts are produced for the test periods. 

\begin{table}[!htp]
\centering\footnotesize\tabcolsep=0.12cm
\caption{Class labels}
\label{classlabels}
\begin{tabular}{llrrrr}
class label & Description & Y & Q/M & W & D/H \\ \hline
WN & white noise process & \checkmark & \checkmark & \checkmark & \checkmark \\
AR/MA/ARMA & AR, MA, ARMA processes & \checkmark & \checkmark & \checkmark & -\\
ARIMA & ARIMA process & \checkmark & \checkmark & \checkmark & - \\
SARIMA & seasonal ARIMA & \checkmark & \checkmark & \checkmark & -\\
RWD & random walk with drift & \checkmark & \checkmark & \checkmark & \checkmark \\
RW & random walk & \checkmark & \checkmark & \checkmark & \checkmark  \\
Theta & standard theta method & \checkmark & \checkmark & \checkmark & \checkmark \\
STL-AR &  & - & \checkmark & \checkmark & \checkmark \\
ETS-notrendnoseasonal & ETS without trend and seasonal components & \checkmark & \checkmark & \checkmark & - \\
ETStrendonly & ETS with trend component and without seasonal component & \checkmark & \checkmark & \checkmark & -\\
ETSdampedtrend & ETS with damped trend component and without seasonal component  & \checkmark &  \checkmark & - & - \\
ETStrendseasonal & ETS with trend and seasonal components & - & \checkmark & - & - \\
ETSdampedtrendseasonal & ETS with damped trend and seasonal components & - & \checkmark & - & -\\
ETSseasonalonly & ETS with seasonal components and without trend component & -  & \checkmark & - & - \\
snaive & seasonal naive method & \checkmark & \checkmark & \checkmark & \checkmark \\
tbats & TBATS forecasting & - & \checkmark & \checkmark & \checkmark \\
nn & neural network time series forecasts & \checkmark & \checkmark & \checkmark & \checkmark \\
mstlets &  & - & - & \checkmark & \checkmark \\
mstlarima & & - & - & - & \checkmark \\\hline
\end{tabular}
\end{table}

According to the @M4compguide, in the M4-competition the forecast accuracy is evaluated based on the mean Absolute Scaled Error (MASE) and the symmetric Mean Absolute Percentage Error (MAPE). Hence, in order to identify the "best" forecast-model for each time series in the reference set we combine MASE and the symmetric MAPE calculated over the test set. More specifically, for each series both forecast error measures MASE and sMAPE are calculated for each of the forecast models. Each of these is respectively standardized by the median MASE and median sMAPE calculated across the forecast-models. The model with the lowest average value of the scaled MASE and scaled sMAPE is selected as the output class-label. 


### Train a random forest classifier

A random forest algorithm is used to train the meta-learner. We build separate random forest classifiers for yearly, quarterly, monthly, weekly, daily and hourly time series. The wrapper function called `build_rf` in the `seer` package (available at: https://github.com/thiyangt/seer) enables the training of a random forest.

## FFORMS framework: online phase

The online phase of the algorithm involves generating point forecasts and 95% prediction intervals for the new series or the future values observed time series. First, the corresponding features are calculated based on the full length of the training period provided by the M4 competition. Second, point forecasts and 95% prediction intervals are calculated based on the predicted class labels. We should note that all negative forecasts are set to zero.

# Peeking inside FFORMS{#machinelearning}

The main objective of this paper is to explore the nature of the relationship between features and forecast-model selection learned by the FFORMS framework. More specifically, to identify which of the features are important for model predictions and how different features and their interactions led to the different choices. We use both model-diagnostic approaches and machine learning interpretability approaches to evaluate our framework.

## Machine Learning Interpretability

In recent years, there have been a growing interest for interpretability of machine learning algorithms with the European General Data Protection Regulation (GDPR) stipulates the explainability of all automatically made decision concerning individuals. We explore the role of features in two different perspectives: i) global explanation of feature contribution: overall role of features in the choice of different forecast model selection, and ii) local explanation of feature contribution: nature of the contributions features make for a prediction of a specific instance. We will introduce each of these ideas briefly below. 

## General Notation

Let \(\mathcal{P}=\{(\mathbf{x^{(i)}}, y^{(i)})\}_{i=1}^{N}\) be the
historical data set we use to train a classifier. Consider a
p-dimensional feature vector \(X=(X_1, X_2, ..., X_p)\) and a dependent
variable, the best forecasting method for each series \(Y\). Let \(\mathcal{G}\) be the unknown relationship between \(X\) and
\(Y\). @Zhao term this as "law of nature". Inside the FFORMS framework, random forest algorithm tries to learn this relationship using
the historical data we provided. We denote the predicted function as
$g$.

## Model-diagnostics

Model-diagnostic is an important aspect in evaluating the accuracy of the model's predictions as well as the model's understanding of the nature of the relationship between features and predicted outcome. It is argued in order to estimate the test error of a bagged model it is not necessary to perform cross-validation approach, because each tree is grown using different bootstrap samples from  the training set and a part of training data is not used in the tree construction (@breiman2001random; @chen2004using). In general, each bagged tree does not make use of around one third of observations to construct the decision tree. These observations are referred to as the out-of-bag (OOB) observations. Each tree is grown based on different bootstrap samples hence, each tree has different set of OOB observations. These OOB samples are used to calculate internal estimation of the test set error. We use vote matrix calculated based on OOB observations as a model-diagnostic tool. The vote matrix ($N \times P$; $N$ is total number of observations, $P$ is number of classes) contains the proportion of times each observation was classified to each class based on OOB sample.

## Representation of model in the data space (m-in-ds) and data in the model space (d-in-ms)

@wickham2015visualizing explains the importance of displaying the "model in the data space (m-in-ds)" and "data in the model space (d-in-ms)". Displaying the data in the model space (d-in-ms) is the most commonly used approach for model-diagnostics. For example, plot of fitted values versus residuals (@wickham2015visualizing). D-in-MS is a visualization of embedding high-dimensional data into a low-dimensional space generated from the model. Visualization of D-in-MS do not help to
gain an understanding of the nature of the relationship between features predicted outcome. In order to address this issue @wickham2015visualizing and @da2017interactive have highlighted the importance of visualizations of model in the data space. In the context of classification, representation of m-in-ds could be achieved by first, projecting the training data set into meaningful low-dimensional feature space and then visualize the complete prediction regions or their boundaries. In other words, this can be considered as the visualization of predictor space in the context of the data space. See @wickham2015visualizing for visualization method of this kind and @da2017interactive for comparable method for random forest algorithm. 


## Global Interpretability Methods

Global interpretability evaluates the behavior of a model on entire data set. Global perspective of model interpretation helps users to understand the overall modelled relationship between features and the model outcome. For example, which features are contributing mostly to the predictive mechanism of the fitted model, complex interactions between features, etc. In the following subsections, we provide a description of tools we use to explore the global perspective of the model.  

## Analysis of Feature contribution

@jiang2002 explains variable importance under three different views: i) causality: change in the value of Y for an increase or decrease in the value of x, ii) contribution of X based on out-of-sample prediction accuracy and iii) face value of X on prediction function $g$, for example in linear regression model estimated coefficients of each predictor can be considered as a measure of variable importance. See @jiang2002 for comparable face value interpretation for machine learning models. In this paper we use the first two notions of variable importance. Partial dependency functions and individual conditional expectation curves are used to explore the "causality" notion of variable importance while Mean decrease in Gini coefficient and Permutation-based variable importance are used to capture the second notion of variable importance-features contribution to the predictive accuracy (@Zhao). We will introduce each of these variable importance measures below.  

### Mean decrease in Gini coefficient

Mean decrease in Gini coefficient is a measure of how each feature contributes to the homogeneity of the nodes and leaves in the resulting random forest proposed by @breiman2001random.


### Permutation-based variable importance measure

The permutation-based variable importance introduced by @breiman2001random measures the prediction
strength of each feature. This measure is calculated based on the  out-of-bag (OOB) observations. The calculation of variable importance is formalized as follow: Let $\bar{\mathcal{B}}^{(k)}$ be the OOB sample for a tree $k$, with $k\in \{1,...,ntree\}$, where $ntree$ is the number of trees in the random forest. Then the variable importance of variable $X_{j}$ in $k^{th}$ tree is:
 \[VI^{(k)}(X_{j})=\frac{\sum_{i\in \bar{\mathcal{B}}^{(k)}}I(\gamma_{i}=\gamma_{i,\pi_{j}}^{k})}{|\bar{\mathcal{B}}^{(k)}|}-\frac{\sum_{i\in \bar{\mathcal{B}}^{(k)}}I(\gamma_{i}=\gamma_{i}^{k})}{|\bar{\mathcal{B}}^{(k)}|},\]
 where $\gamma_{i}^{k}$ denotes the predicted class for the $i^{th}$ observation before permuting the values of $X_{j}$ and $\gamma_{i, \pi_{j}}^{k}$ is the predicted class for the $i^{th}$ observation after permuting the values of $X_{j}$. The overall variable importance score is calculated as:
 \[VI(X_{j})=\frac{\sum_{1}^{ntree}VI^{(t)}(x_{j})}{ntree}.\]

Permutation-based variable importance measures provide a useful starting point for identifying relative influence of features on the predicted outcome.  However, they provide a little indication of the nature of the relationship between the features and model outcome. To gain further insights into the role of features inside the FFORMS framework we use partial dependence plot (PDP) introduced by @friedman2008predictive. 

### Partial dependence plot (PDP)

Partial dependence plot can be used to graphically examine how each feature is related to the model prediction while accounting for the average effect of other features in the model. Let $X_s$ be the subset of feature we want examine partial dependencies for and $X_c$ be the remaining set of features in $X$.  Then $g_s$, the partial dependence function on $X_s$ is defines as 
\[g_s(X_s)=E_{x_c}[g(x_s, X_c)]=\int{g(x_s, x_c)dP(x_c).}\] 
In practice, PDP can be estimated from a training data set as 
\[\bar{g_s}(x_s)=\frac{1}{n}\sum_{i=1}^{n}g(x_s, X_{iC}),\]
where $n$ is the number of observations in the training data set. Partial dependency curve can be created by plotting the pairs of $\{(x_s^k, \bar{g}_s(x_{sk}))\}_{k=1}^{m}$ defined on grid of points $\{x_{s1}, x_{s2},\dots, x_{sm}\}$ based on $X_s$. FFORMS framework has treated the forecast-model selection problem as a classification problem. Hence, in this paper partial dependency functions displays the probability of certain class occurring given different values of the feature $X_s$.

### Variable importance measure based on PDP

@Greenwell2018 introduced a variable importance measure based on the partial dependency curves. The idea is to measure the “flatness” of partial dependence curves for each feature. A feature whose PDP curve is flat, relative to the other features, indicates that the feature does not have much influence on the predicted value as it changes while taking into account the average effect of the other features in the model. The flatness of the curve is measured using the standard deviation of the values $\{\bar{g}_{s}(x_{sk})\}_{k=1}^{m}$.

### Individual Conditional Expectation (ICE) curves

While partial dependency curves are useful in understanding the estimated relationship between features and the predicted outcome in the presence of substantial interaction between features, it can be misleading.  @goldstein2015peeking proposed the Individual Conditional Expectation (ICE) curves to address this issue. Instead of
averaging $g(x_s, X_{iC})$ over all observations in the training data, ICE plots the individual response curves by plotting the pairs $\{(x_s^k, g(x_{sk}, X_{iC}))\}_{k=1}^{m}$ defined on grid of points $\{x_{s1}, x_{s2},\dots, x_{sm}\}$ based on $X_s$. In other words, partial dependency curve is simply the average of all the ICE curves. 


### Variable importance measure based on ICE curves

This method is similar to the PDP-based VI scores above, but are based on measuring the “flatness” of the individual conditional expectation curves. We calculated standard deviations of each ICE curves.  We then computed an ICE based variable importance score – simply the average of all the standard deviations. A higher value indicates a higher degree of interactivity with other features. 

## Assessment of Interaction Effect

Friedman's H-statistic (@friedman2008predictive) is used to test the presence of interaction between all possible pairs of features. This statistic is computed based on the partial dependence functions. For two-way interaction between two specific variable $x_j$ and $x_k$, Friedman's H-statistic is defined as follow,

\[H_{jk}^2=\sum_{i=1}^{n}[\bar{g}_{s}(x_{ij}, x_{jk})-\bar{g}_{s}(x_{ij})-\bar{g}_{s}(x_{ik})]^2/\sum_{i=1}^{n}\bar{g}^2_{s}(x_{ij}, x_{jk}).\]

The Friedman's H-statistic measures the fraction of variance of two-variable partial dependency, $\bar{g}_{s}(x_{ij}, x_{jk})$ not captured by sum of the respective individual partial dependencies, $\bar{g}_{s}(x_{ij})+\bar{g}_{s}(x_{ik})$. In addition to Friedman's H-statistic we also use the PDP of two variables to visualize the interaction effects. 

Note that the, PD plots, ICE curves and PD-, ICE-associated measures and Friedman's H-statistic are computationally intensive to compute, especially when there are large number of observations in the training set. Hence, in our experiments ICE and PDP-based variable importance measures are computed based on the subset of randomly selected training examples.


## Local Interpretable Model-agnostic Explanations (LIME)

Global interpretations help us to understand the entire modelled relationship.  Local interpretations help us to understand the predictions of the model for a single instance or a group of similar instances. In other words this allows users to zoom into a particular instance or a subset and explore how different features affect the resulting prediction. We use Local Interpretable Model-agnostic Explanations (LIME) approach introduce by @ribeiro2016should for explaining individual predictions which relies on the assumption that "every complex model is linear on a local scale". This is accomplished by locally approximating the complex black-box model with a simple interpretable model. @ribeiro2016should highlighted features that are globally important may not be important in the local context and vice versa. The algorithm steps can be summarized as follow: 

1. Select an observation of interest which we need to have explanations for its black-box prediction.
2. Create a permuted data set based on the selected observation. Permuted data set is created by making slight modifications to the features of selected observations.
3. Obtain similarity scores by calculating distance between permuted data and selected observation.
4. Obtain predicted outcomes for all permuted data using the black-box model.
5. Select $m$ number of features best describing the black-box model outcome. This can be accomplished by applying feature selection algorithms such as ridge regression, lasso, etc.
6. Fit a simple linear model to the permuted data based on $m$ selected features, similarity scores in step 3 as weights and complex model prediction outcomes in step 4 as response variable.
7. Use the estimated coefficients of simple linear model to explain the local behaviour corresponds to the selected observation in step 1.

An alternative for explaining local behaviour of complex models is proposed by @lundberg2017unified based on game theory named "Shapley values".

# Results{#results}

## Yearly data

Random forest returns a vote matrix which is useful in evaluating the uncertainty of observations. The vote matrix ($N \times P$; $N$ is total number of observations; $P$ is number of classes) contains the proportion of times each observation was classified to each class based on OOB sample. This information for yearly data random forest is presented in \autoref{fig:yearlyoob}. This is an alternative way of visualizing the vote-matrix information. The other way of representing vote matrix involves ternary plot (@sutherland2000orca) and jittered side-by-side dotplot [@ehrlinger2015ggrandomforests; @da2017interactive]. To overcome the problem of overlapping data points due to the scale of the training data set, high similarity between classes and relatively large number of class labels, boxplot diagrams are used. \autoref{fig:yearlyoob} helps to evaluate the model performance in the data space (model-in-the-data-space) (@da2017interactive).

According to \autoref{fig:yearlyoob} the distributions of correctly classified classes dominate, indicating a good classification of the meta-learner. The random walk with drift has a high chance of getting selected with yearly time series and the results of M4-competition also shows random walk with drift perform well with yearly time series.  The forecast-models, neural network and theta also have a high chance of getting selected as they represent a more general class of forecast-models. Furthermore, FFORMS framework successfully learnt the similarities and dissimilarities between the classes itself.  For example, within ETS-trend predicted class, the distributions correspond to the class labels, ETS-damped trend, ARIMA, were also assigned with high probability and less values were assigned to ARMA/AR/MA, White noise process and ETS (ANN)/ ETS(MNN).


```{r functionfriedman}
```


```{r yearlyoob, fig.height=10, fig.width=10, fig.cap="Visualization of the vote matrix based on OOB sample for yearly random forest. The Y-axis denotes what was predicted from the random forest. The X-axis denotes the proportion of times each time series was classified to each class. The colours of boxplots corresponds to class label of \"best\" forecast-model identified based on MASE and sMAPE. On each row, distribution of correctly classified class dominates, indicating a good classification of the meta-learner.", fig.pos="h"}
```


\autoref{fig:viyearly} shows how important each one of the features we considered within each class as well as in the overall classification mechanism of the FFORMS. To identify class-specific important features we rank features in three different ways: i) based on permutation-based variable importance, ii) based on partial dependence functions and iii) based on ICE-curves. We consider 25 features for yearly data. The feature that shows the highest importance is ranked 25, the second best is ranked 24, and so on. Finally, for each feature, a mean rank is calculated based on the rankings of the three measures. Similarly, the overall feature importance is evaluated based on permutation-based variable importance measure and Gini coefficient-based feature importance measure.

According to \autoref{fig:viyearly}, the features related to strength of trend, nonstationarity (ur_pp, diff1y_acf1), overall shape of the trend (linear: measured by linearity, damped trend: measured by beta, exponential: measured by curvature) and measures of randomness (from spikiness, and lmres_acf1) are the most important for the choice of yearly time series forecast-models. The first ACF value of the original series (y_acf1) appears among top five within random walk with drift class and ARMA/AR/MA class as it helps to separate stationary and non-stationary series. The first correlation coefficient of the twice-differenced series is appear to be the most important in ARIMA class as this class contains the higher order differenced series.  The Hurst exponent and entropy appear to be equally important in stationary classes. Within ETS-damped trend class beta and curvature ranked as important features. The length of time series (N) is assigned relatively high rank within random walk with drift, ETS-dampedtrend and neural-network class compared to others. On the other hand, sum of squares of first five autocorrelation coefficients of the twice-difference series and lumpiness are the least important feature across many classes.

 
\autoref{fig:pdpyearly} shows the partial dependency curves, and associated confidence intervals of the top-three features that get selected most in each class. The three features show a non-linear relationship with predicted class probabilities. Probability of selecting ETS-trend, ARIMA, ETS-without seasonal and trend component and neural network models increases steadily as ur_pp increases. As expected, probability of selecting stationary models decreases as the test statistic of Phillip-Perron test increases and this probability remains zero  beyond the value of 0 of ur_pp. Random walk with drift, ETS-trend, ETS-damped trend, ARIMA show an increasing relationship with trend, whereas random walk, ETS-without trend and seasonal components, and stationary models show a monotonically deceasing relationship as  trend increases. The theta class shows parabolic relationship with trend. It is interesting to observe that probability of selecting neural network models decreases with very high trend value. The reason could be very clear highly trended series are more likely to select ETS-trend, ETS-dampedtrend and ARIMA models which lessen the chance of neural network models for them. The wide confidence bands around the partial dependency functions of linearity indicated the higher variability of ICE curves. The probability of selecting random walk with drift increases rapidly beyond value 0 and remains thereafter. The PDP curve of linearity in ARMA/AR/MA increase sharply around 0 and decline steadily after that. Similar relationship can be observed within white noise class with wide confidence bands whereas ARIMA and neural network show the opposite relationship. The partial dependency curves of diff1y_acf1 indicate probability of selecting random walk with drift and ETS-trend higher for differenced-stationary series. 

\autoref{fig:friedmany} shows the heat maps of relative strength of all possible pairwise interactions for each class. The relative strength of two-way interaction between features are measured using the formula developed by @friedman2008predictive, which is implemented in the `iml` (@molnar2018iml,) package in R. Except ETS-trend class, trend and ur_pp show a high level of interactivity and less interactivity with other features. Linearity also show a weak interaction  with other features in all classes.  In almost all the cases partial correlation and auto-correlation based features are heavily interacting. However, the first correlation coefficient of the difference series do not interact with other features heavily within ARIMA  class. Further, almost all pair of features appear to be interacting within neural network category. The interactivity between stability and lumpiness is the most common type of interactivity appear within all classes. 

```{r viyearly, fig.height=15, fig.width=20, fig.cap="Feature importance plot for yearly series. Permutation-based variable importance measure and mean decrease in Gini coefficients are used to evaluate overall feature importance shown in the top left plot. Class-specific feature importance is evaluated based on three measures: i) permutation-based variable importance, PD-based variable importance measure, and ICE-based variable importance measure. Longer bars indicate more important features. Top 5 features are highlighted in red. Strength of trend appear to be the most important feature.", fig.pos="h"}
```

\newpage

```{r pdpyearly, fig.height=28, fig.width=20, fig.cap="Partial dependence plots for the top-three features get selected most within each class for yearly series. The shading shows the 95% confidence intervals. Y-axis denotes the probability of belong to corresponding class. All features show a nonlinear relationship with predicted probabilities.", fig.pos="h"}
```

\newpage

```{r friedmany, fig.height=30, fig.width=20, fig.cap="Heat maps of relative strength of all possible pairwise interactions calculated based on Friedman's H-statistic (yearly series). Strength of trend shows less interactivity with other features.", fig.pos="h", message=FALSE, warning=FALSE, dev='pdf'}
```

\newpage

## Quarterly and Monthly data

\autoref{fig:oobquarterlymonthly1} - \autoref{fig:oobquarterlymonthly2} show the vote-matrix of random forests for quarterly and monthly data respectively based on OOB observations. \autoref{fig:oobquarterlymonthly1} and \autoref{fig:oobquarterlymonthly2} depicted similar patterns across classes. For quarterly and monthly data, the same set of features and class-labels are used to train the model. Hence, this consistency between the results of quarterly and monthly series would provide evidence in support of the validity and trustability of the model. The outliers associated with dominating distributions indicates some series are correctly classified with very high probability. Seasonal time series has a low chance of classified into random walk with drift model and high chance of selecting SARIMA, stlar and tbats. Except the time series labelled as ARMA/AR/MA all other quarterly time series have a very low chance of classified to ARMA/AR/MA class. Further, all distributions correspond to the tbats row located further away from zero. This indicates all-time series select tbats model at least once from the individual trees in the forest. Except few outliers,  distributions within neural network category also show a slight upward deviation from zero. However, the upper boundary of these distributions do not surpass the upper boundaries of dominating box plots in the random walk with drift class and SARIMA class. Further, within stlar, tbats, theta and neural network classes all distributions level at similar proportionalities.  These types of similarities in the distributions indicates the  appropriateness of using combination forecasting.  Further, these information are useful in identifying potential time series models for combination forecast and improve the existing combination approaches proposed in the M4-competition (@Makridakis2018dx). In addition to that the similarities and diversities observed in the boxplots indicate the neighbourhood of cases in their respective instance space.

\autoref{fig:viquarterly} and \autoref{fig:vimonthly} show feature importance plots for quarterly and monthly data respectively. For both quarterly and monthly data strength of seasonality, trend, linearity and spikiness are the most important features across all categories. Even though the lumpiness does not appear as a top five feature within classes it is appeared to be an important feature in the overall classification process and a relatively high ranks are assigned within many classes. In the case of yearly data low variable importance is assigned to both stability and length of the series. However, in quarterly and monthly data high variable importance is assigned to length of the series and stability. One notable difference between quarterly series and monthly series is, for monthly data length of the series is ranked among top five specially in random walk with drift, random walk, ETS with seasonal and trend component, ETS-seasonal, SARIMA and ARIMA classes. In addition to the strength of seasonality, the models available for handling seasonal components (snaive, SARIMA, all ETS models with seasonal component) assigned a high importance to the additional features related to seasonality such as ACF, PACF-based features related to seasonal lag or seasonally differenced series. Furthermore, as expected features calculated based on parameter estimated of ETS(A, A, A) have been ranked as important for the choice of ETS with damped trend and seasonal component and ETS with trend and seasonal component. 

\autoref{fig:pdpquarterly1} and \autoref{fig:pdpquarterly2} show the partial dependency functions of the features that get selected most often in the top. Additionally, PDP of N is included to observe the effects stated in the
literature [@makridakis2000m3]. Except for random walk, partial dependency curves of seasonality and trend show a similar behaviour for both quarterly and monthly data. Hence, for seasonality and trend, the partial dependency curves computed based quarterly are presented except for random walk. Probability of selecting a model with a parameter to handle the seasonal effect (snaive, all ETS models with seasonal component, SARIMA, tbats, theta, stlar) increases as the seasonality increases. Further, for classes rwd, all ETS model with trend component, SARIMA, ARIMA, tbats and theta, have a high probability of getting selected as the strength of trend increases. On the other hand, opposite relationships were observed for snaive and ETS-seasonal which accounted seasonality only. This confirms the idea that the choice of model selection consistent with the expected relationship. FFORMS framework for quarterly series show probability of selecting random walk models remains stable up to 0.85 value of trend and drops sharply afterwards, whereas the FFORMS framework for monthly series show probability of selecting random walk models increases as trend increases. This could be due to the interaction effect of trend with other features. 

\autoref{fig:friedmanQ} and \autoref{fig:friedmanM} show the heat maps of relative strength of all possible pairwise interactions calculated based on Friedman's H-statistic for quarterly and monthly data respectively. Within random walk with drift class trend shows a high interactivity with other features. Within all classes seasonality show less interactivity with other features, whereas lumpiness show high interactivity with other features. Further within each class, a subset of ACF/PACF-based features show some interactivity. In general interactivity between features related to correlation structure of a time series and overall shape (spikiness, linearity, curvature, etc) lead to the choice of forecast-model selection.


```{r oobquarterlymonthly1, fig.height=30, fig.width=25, fig.cap="Visualization of the vote matrix based on OOB sample for quarterly random forest. The Y-axis denotes what was predicted from the random forest?. The X-axis denotes the proportion of times each time series was classified to each class. The colours of boxplots corresponds to class label of \"best\" forecast-model identified based on MASE and sMAPE. On each row, distribution of correctly classified class dominates, indicating a good classification of the meta-learner. ARMA/AR/MA has a low chance of being selected while random walk with drift has a high chance of being selected.", dev='png',fig.pos="h"}
```

\clearpage

```{r oobquarterlymonthly2, fig.height=38, fig.width=30, fig.cap=" Visualization of the vote matrix based on OOB sample for monthly random forest. The Y-axis denotes what was predicted from the random forest? The X-axis denotes the proportion of times each time series was classified to each class. The colours of boxplots corresponds to class label of \"best\" forecast-model identified based on MASE and sMAPE. On each row, distribution of correctly classified class dominates, indicating a good classification of the meta-learner. Few series correspond to stlar and nn have been correctly classified with a very high probability.", dev='png',fig.pos="h"}
```

\clearpage

```{r viquarterly, fig.height=11, fig.width=23, fig.cap="Feature importance plot for quarterly data. Permutation-based VI measure and mean decrease in Gini coefficients are used to evaluate overall feature importance. Class-specific feature importance is evaluated based on the three measures: permutation-based VI, PD-based VI measure, and ICE-based VI measure. Longer bars indicate more important features. Top 5 features are highlighted in red.", fig.pos="h", fig.align="center",dev='pdf'}
```

```{r vimonthly, fig.height=11, fig.width=23, fig.cap="Feature importance plot for monthly data. Permutation-based VI measure and mean decrease in Gini coefficients are used to evaluate overall feature importance. Class-specific feature importance is evaluated based on the three measures: permutation-based VI, PD-based VI measure, and ICE-based VI measure. Longer bars indicate more important features. Top 5 features are highlighted in red.", fig.pos="h", fig.align="center", dev='pdf'}
```

\newpage

```{r pdpquarterly1, fig.height=28, fig.width=20, fig.cap="Partial dependence plots for the top-three features get selected most within each class inside quarterly and monthly FFORMS frameworks. Additionally, N is included to observe the effects stated in the literature. The shading shows the 95% confidence interval. Y-axis denotes the probability of belonging to corresponding class. Red colour is for PDP drawn based on quarterly data and blue colour is for the PDP drawn based on monthly data.", fig.pos="h"}
```

\newpage

```{r pdpquarterly2, fig.height=28, fig.width=20, fig.cap="Partial dependence plots for the top-three features get selected most within each class inside quarterly and monthly FFORMS frameworks. Additionally, N is included to observe the effects stated in the literature. The shading shows the 95% confidence interval. Y-axis denotes the probability of belonging to corresponding class. Red colour is for PDP drawn based on quarterly data and blue colour is for the PDP drawn based on monthly data.(Continue from Figure 9)", fig.pos="h"}
```

\newpage

```{r friedmanQ, fig.height=30, fig.width=20, fig.cap="Heat maps of relative strength of all possible pairwise interactions calculated based on Friedman's H-statistic for quarterly data.", fig.pos="h", message=FALSE, warning=FALSE, dev='pdf'}
```

\newpage

```{r friedmanM, fig.height=30, fig.width=20, fig.cap="Heat maps of relative strength of all possible pairwise interactions calculated based on Friedman's H-statistic for monthly data.", fig.pos="h", message=FALSE, warning=FALSE, dev='pdf'}
```

## Weekly

\autoref{fig:oobweekly} shows the proportion of times each time series was classified to each class. Unlike, yearly, quarterly and monthly data theta method has a low chance of getting selected. The random walk with drift, tbats models and nn have a high chance of getting selected. Except ARMA/AR/MA class the distribution corresponds to the true class label dominates others. ARMA/AR/MA class shows some unusual behaviour within some categories due to class imbalance ratio, ARMA/AR/MA class contains fewer number of observations in the training set. 

```{r oobweekly, fig.height=9.5, fig.width=10, fig.cap="Visualization of the vote matrix based on OOB sample for weekly random forest. The Y-axis denotes what was predicted from the random forest. The X-axis denotes the proportion of times each time series was classified to each class. The colours of boxplots corresponds to class label of \"best\" forecast-model identified based on MASE and sMAPE. The models rwd, tbats, nn have a high chance of getting selected.", fig.pos="h"}
```

\clearpage

According to the results of \autoref{fig:viweekly} spikiness, linearity, trend, strength of seasonality, stability and lumpiness have been assigned a high importance. This is similar to the results of yearly, quarterly and monthly data. The length of series has been selected among top 5 by mstlets, tbats, theta and neural network models. According to the results of \autoref{fig:weeklypdp} for mstlets models probability of being selected increases as the linearity increases while the opposite relationship was observed at SARIMA models. According to \autoref{fig:weeklypdp} probability of selecting snaive, random walk, neural network and white noise increases as spikiness increases. Furthermore, as expected, partial dependency plots reveal probability of selecting random walk and neural network models decrease as the stability increases while the opposite relationship can be observed for white noise class.

```{r viweekly, fig.height=15, fig.width=20, fig.cap="Feature importance plot for weekly data. Permutation-based VI measure and mean decrease in Gini coefficient is used to evaluate overall feature importance. Class-specific feature importance is evaluated based on the three measures: permutation-based VI, PD-based VI measure, and ICE-based VI measure. Longer bars indicate more important features. Top 5 features are highlighted in red.", fig.pos="h", fig.align="center"}
```

\newpage

```{r weeklypdp, fig.height=28, fig.width=20, fig.cap="Partial dependence plots for the top ranked features from variable importance measures (weekly series). The shading shows the 95% confidence intervals. Y-axis denotes the probability of belong to corresponding class.", fig.pos="h"}
```


\newpage

## Daily and Hourly data

According to \autoref{fig:oobdailyhourly} the distributions corresponds to observations that have been correctly classified dominated the top for daily data.  However, within daily series there are few observations that have been incorrectly classified to tbats class with very high probabilities. In general, neural network models have higher chance of getting selected for daily time series. Overall, for hourly series random walk with drift models, tbats and neural network models have high chance of getting selected. Furthermore,  it is important to note that all hourly series have been assigned non-zero probability of getting selected to neural network class. 

\autoref{fig:dailypdp} shows the partial dependency plots of the
top 3 features from the FFORMS framework. According to the results of
\autoref{fig:dailypdp} shorter series tends to select random walk with
drift models while probability of selecting snaive, mstlarima and mstlets models
increases as the length of series increases. Neural network models show
a non-monotonic relationship with length of the series (N). The theta
models tend to be selected for series with high annual seasonality but
very low weekly seasonality.

Variable importance graph for daily and hourly data are shown in \autoref{fig:vidaily} and \autoref{fig:vihourly} respectively. The most
important features for determining suitable forecast-models for daily
time series are, strength of seasonality corresponds to the weekly
seasonality (7, measured by seasonal_strength1), stability, trend, lumpiness and linearity. Furthermore, length of the series is important in determining random walk,
random walk with drift, mstlarima, mstlets, stlar, theta and nn classes.
 
According to \autoref{fig:vihourly}, strength of daily
seasonality (period=24, measured by seasonal_strength1) appear to be more important than the strength of weekly seasonality (period=168, measured by seasonal_strength2). Furthermore, entropy, linearity, sum of squares of first 5 coefficients of PACF, curvature, trend, spikiness and stability were found to be the most important features in determining best forecasting method for hourly time series. Only snaive category ranked N among top 5 for hourly time series. The strength of weekly seasonality also seems to be one of the most important features for the classes snaive, random walk, mstlarima, and tbats. 

According to \autoref{fig:hourlypdp} probability of selecting random walk, random
walk with drift, theta model and white noise process decreased with
higher strength of daily (seasonal_strength1) seasonality, while the opposite relationship holds for other classes. On the other hand, probability of selecting
random walk model increased with the increase in strength of weekly
seasonality. 

```{r oobdailyhourly, fig.height=20, fig.width=15, fig.cap="Visualization of the vote matrix based on OOB sample for daily and hourly random forests. The Y-axis denotes what was predicted from the random forest. The X-axis denotes the proportion of times each time series was classified to each class. The colours of boxplots corresponds to class label of \"best\" forecast-model identified based on MASE and sMAPE. On each row, distribution of correctly classified class dominates, indicating a good classification of the meta-learners.", fig.pos="h"}
```

\newpage

```{r vidaily, fig.height=11, fig.width=23, fig.cap="Feature importance plot for daily data. Permutation-based VI measure and mean decrease in Gini coefficients are used to evaluate overall feature importance. Class-specific feature importance is evaluated based on the three measures: permutation-based VI, PD-based VI measure, and ICE-based VI measure. Longer bars indicate more important features. Top 5 features are highlighted in red.", fig.pos="h", fig.align="center"}
```


```{r vihourly, fig.height=11, fig.width=23, fig.cap="Feature importance plot hourly series. Permutation-based VI measure and mean decrease in Gini coefficients are used to evaluate overall feature importance. Class-specific feature importance is evaluated based on the three measures: permutation-based VI, PD-based VI measure, and ICE-based VI measure. Longer bars indicate more important features. Top 5 features are highlighted in red.", fig.pos="h"}
```


\newpage

```{r dailypdp, fig.height=28, fig.width=20, fig.cap="Partial dependence plots for the top ranked features based on variable importance measures (daily series). The shading shows the 95% confidence intervals. Y-axis denotes the probability of belong to corresponding class. (seasonal_strength1 denotes weekly seasonality and seasonal_strength2 for annual seasonality).", fig.pos="h"}
```

\newpage

```{r hourlypdp, fig.height=28, fig.width=20, fig.cap="Partial dependence plots for the top ranked features from variable importance measures (hourly series). The shading shows the 95% confidence intervals. Y-axis denotes the probability of belong to corresponding class. (seasonal_strength1 denotes daily seasonality and seasonal_strength2 for weekly seasonality)", fig.pos="h"}
```

According to Friedman's H-statistic for daily series within each category, sediff_acf5 and weekly seasonality (seasonal_strength2) show high interactivity within each class, while for hourly series sediff_seacf1 and linearity show high interactivity within each class. The partial dependency plots of associated figures are shown in \autoref{fig:dtwopdp} and \autoref{fig:htwopdp}. Each plot shows a unique pattern of interactivity which are useful in separating one from another. 

```{r dtwopdp, fig.height=10, fig.width=20,  fig.pos="h", message=FALSE, warning=FALSE, fig.cap="Partial dependence plot of model selection probability and the interaction of sediff_acf5 and seasonal_strength2 for daily data. Dark regions show the high probability of belonging to the corresponding class shown in the plot title. Within each class unique pattern of interaction pattern exist between sediff_acf5 and seasonal_strength2. ", dev='png'}

```

```{r htwopdp, fig.height=10, fig.width=20,  fig.pos="h", message=FALSE, warning=FALSE, fig.cap="Partial dependence plot of model selection probability and the interaction of sediff_seacf1 and linearity for hourly data. Dark regions show the high probability of belonging to the corresponding class shown in the plot title. Random walk and random walk with drift class show opposite pattern of interactivity between sediff_seacf1 and linearity.", dev='png'}

```


\clearpage

## Local Interpretable Model-agnostic Explanations

```{r quarterlylime, fig.height=8, fig.width=20, fig.cap="", fig.pos="h", fig.align="center",  dev='png'}
```

```{r quarterlylime2, fig.height=8, fig.width=15, fig.cap="Panel A: Distribution of quarterly time series in the PCA space. Panel B: Time series corresponds to the highlighted points in the PCA space. Panel C: Local interpretable Model-agnostic explanations for four selected quarterly time series. Features denoted with green colour are supporting features for an outcome label and length of the bar is proportional to the weight of a feature.", fig.pos="h", fig.align="center", cache=FALSE, dev='pdf'}
```

We now illustrate how LIME approach can be used to zoom into local regions of the data to identify which features, contribute most to classify a specific instance. For the illustration we select for different types of time series classified with high probability. \autoref{fig:quarterlylime2} shows the feature contribution for the instances highlighted on the PCA-space of quarterly series. We can see how the different strength of seasonality influence the FFORMS framework to select different types of seasonal  forecast-models. For example, SARIMA models was selected when the seasonality varies between 0.579 and 0.787 (case 1), ETS-seasonal models is selected when the strength of seasonality is greater than 0.787 (case 4), random walk with drift when the seasonality with lower than 0.579 (case 2) and for the highly trended and seasonal series (strength of seasonality > 0.895) ETS model with a trend and seasonal component is selected (case 3). Further, high value of diff1y_acf5 in supports the selection of SARIMA for case 1 while, moderate value of diff1y_acf5 supports the selection of ETS-seasonal for the case 4. Similarly, we can explore the reasons for other instances in all frequency categories. From LIME approach we can gain insight into the local neighbourhood characteristics which lead to the choice of a particular neighbourhood over alternative destinations.

# Discussion and Conclusions{#conclusions}

Forecast model selection is both time and computer cost intensive process. Consequently, the application of machine-learning approaches to predict suitable forecast-model from large number of potentially relevant time series features is a topic growing popularity in the field of time series forecasting. In this paper we used model-agnostic machine learning interpretability tools to explore what is happening under the hood of FFORMS framework and to gain an understanding of what features led to the choices of FFORMS framework. The results we present here are a novel application  of machine learning interpretability methods to visualize and explore the role of features in forecast-model selection. Furthermore, explaining predictions is an important aspect in getting humans trust and use the proposed framework effectively, if the explanations are faithful and intelligible. Humans usually have prior knowledge about the application domain, which they can use to accept (trust) or reject prediction if they understand the reasons behind it.  

We explore the role of features in two different perspectives: i) individual effect, and ii) interaction effect. Overall, strength of trend, strength of seasonality, linearity, spikiness and curvature rank among the top 10 within each frequency category. @lemke2010meta also pointed out features related to nonstationarity and seasonality of a series are important factors for choosing a forecasting method. Partial dependency plots are used to visualize the learned relationship between features and the model predictions. The displayed relationships confirm to domain knowledge expectations. However, since several numbers of features are used to build the framework with comparable contributions, and thus, all individual contributions are small. According to the results of daily and hourly data we also observed neural network modelling model was appropriate for forecasting high frequency data. In response to the results of the M3-competition this has been pointed out by many commentators [@makridakis2000m3]. Further, our results show that the performance of various methods depend upon the length of the time series. Short time series tends to select  simple methods such as random walk models, snaive, etc. ETS models with both trend and seasonal components, SARIMA models, mstl models tend to provide accurate forecasts with longer time series as these are more parameterized models.   

As FFORMS framework is developed on top of random forest algorithm takes into account every possible interaction. It was apparent from the heat matrices of Friedman's H-Statistic presented that a substantial interaction effect exists between the features. The strength trends showed less interactivity in yearly series data, reflecting that these features are more important on their own. The features involve in interaction and their strength of interaction effect differ across the different frequency categories (yearly, quarterly, monthly, weekly, daily and hourly) as well as forecast-models (random walk, ETS models, etc.). However, it is interesting to note that in each frequency category all or subset of ACF/PACF-based features interact each other. This confirm that information regarding correlation structure of the time series is an essential information for the choice of model selection. A unique pattern of interactivity exists within each class are useful for separating one from another.  

Exploration of conditions learnt by the FFORMS framework also support practitioners to make a good educated guess on suitable forecast-model for a given problem. Further the results of this study is useful in identifying new ways to improve forecasting accuracy by capturing different features of time series.


# References



