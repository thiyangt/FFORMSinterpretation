---
title: "Peeking inside FFORMS: Feature-based FORecast Model Selection"
author:
- familyname: Troy
  othernames: Helen
  address: University of Greece\newline Athens
  email: helen.troy@gmail.com
  correspondingauthor: true
- familyname: Khan
  othernames: Genghis
  address: Mongolian Institute
abstract: "This paper explores the use of feature-based forecast model selection framework to investigate the relationship between features of time series and forecast-model selection. This study investigates the relationship between features of time series and forecast-model selection using FFORMS framework. It is becoming more of a challenge to not only build state-of-the-art predictive models, but also gain an understanding of what's really going on in the data. We explore the impact of features in three levels, globally, subset of observations and individual  level. In this paper we use model-agnostic appraches to explore the impact of time series features towards forecast-model selection. Graphical representations are used to visualize both main and interaction effects."
keywords: "FFORMS, machine learning, interpretability, partia-dependece, time series"
wpnumber: no/yr
jelcodes: C10,C14,C22
blind: true
cover: true
toc: false
bibliography: references.bib
biblio-style: authoryear-comp
output:
  MonashEBSTemplates::workingpaper:
    fig_caption: yes
    fig_height: 5
    fig_width: 8
    includes:
      in_header: preamble.tex
    keep_tex: yes
    number_sections: yes
    citation_package: biblatex
---

```{r setup, include=FALSE}
library(knitr)
knitr::opts_chunk$set(echo = FALSE, cache=TRUE, messages=FALSE, warning=FALSE, fig.path = 'figures/', dev=c('png'), fig.pos= "h", external = TRUE)
# Make sure you have the latest version of rmarkdown and bookdown
#devtools::install_github("rstudio/rmarkdown")
#devtools::install_github("rstudio/bookdown")
read_chunk("src/main.R")
read_chunk("src/main_weekly.R")
read_chunk("src/main_daily.R")
read_chunk("src/main_hourly.R")
read_chunk("src/main_lime.R")
library(ggplot2)
library(patchwork)
library(tidyverse)
library(reshape2)
library(ggplot2)
library(grid)
library(gridExtra)
library(ggrepel)
library(png)
library(tsfeatures)
library(tidyverse)
library(ggpubr)
library(forestviews)
#install_github(repo = 'thiyangt/forestviews')
library(networkD3)
library(RColorBrewer)
library(iml) #machine learning interpretability package
library(ggcorrplot) # to draw  ggcorrplot
library(ggpubr)
library(lime)
```


# Introduction{#intro}

The time series forecasting field has been evolving for a long time and has introduced a wide variety of forecasting methods. However, for a given time series the selection of an appropriate forecasting-method among many possibilities is not straight forward. This selection is one of the most difficult tasks as each method perform best for some but not all tasks. The features of time series are considered to be an important factor in identifying suitable forecasting models[ref, Meade, MakrikadisM3].[This goes back to 1972]. However, a description of relationship between the features and the performance of algorithms is rarely discussed in the field of forecasting. 

There have been several recent studies on use of machine learning algorithms to automate the forecast model selection based on the features computed from the time series. Meta-learning approach provides a systematic guidance on model selection based on knowledge acquire from historical data set, in our case historical collection of time series. The key idea behind these framework is, forecast-model selection is posed as a supervised learning task. Each time series in the meta-data set is represented as a vector of features and labeled according to the "best" forecasting method(i.e. lowest MASE, etc.). Then a meta-learner is trained to identify suitable forecasting models. With the era of big data, such an automated model selection process is necessary because the cost of invoking all possible forecasting method is prohibitive. However, these work suffer from the limitation of providing meaningful interpretations that can enhance understanding of relations between features and model outcomes. To best of our knowledge, very limited efforts have been taken to understand how the models are making its decisions and what is really happening inside these complex model structures.  This results in less transparency of the model which lead to the questions of

\begin{enumerate}
\item How features are related to the property being modeled?
\item How features interact with each other to identify the suitable forecasting method?
\item Why one forecasting method is preferable over other? 
\item How different features contribute to the model prediction or how they affect the model performance?
\item Why certain features were responsible in driving certain decisions?
\item Why is your model less accurate in some areas of the instance space?
\item Which features contribute the most to classify a specific instance?
\end{enumerate}

On the other hand, aside from the goal of developing automated forecast-model selection framework few researchers have made an attempt to provide a description of relationship between the features and the performance of algorithms. One of the first attempts to identify the relationship between features and forecast-model performance was presented in xx. However, these studies are limited by the scale of problem instances used, diversity of forecasting-methods used, quality of features considered, and modelling approached used to identify the relationship between features and forecast model performance. Most of these studies are typically restricted to simple statistical techniques such as...[do not capture the complex interaction effects]


To explore these points further, this paper makes a first step towards providing a comprehensive explanation of the relationship between time series features and forecast-model selection using machine learning interpretability techniques. This paper builds on the method from our previous work ref,  in which we introduced the FFORMS (Feature-based FORecast Model Selection) framework. The random forest algorithm is used to model the relationship between features and "best" performing forecast-model. [Large time series collection is used to train the model] We use 30 features to capture morphology of time series. One of the principal advantages of random forest algorithm  is their ability to model complex variable interactions. One noticeable significance of our approach is this can be parallized to for any given computing budget and time. Even though the prediction accuracy of random forest algorithm is high, it is not easy to interpret what is happening inside the forest because of the two-step randomization.In this work we aim at providing a deeper understanding of the underlying mechanism and influence of features in forecast-model selection.  
 
[What is the usefulness of analyzing the relationship between features and model performance?] Understanding the role of features is worthwhile even if producing an accurate and generalizable model is the only objective of the modelling. This is because the less transparency of the model may be distrusted regardless of their predictive performance. The methodology we propose here is a novel application of machine learning interpretability methods to visualize and explore the role of features in forecast-model selection. 

This paper proceeds as follows. In \autoref{fforms} we describe the application of FFORMS framework to M4competition data. The main contribution of this from our previous work @fforms  is we extend the FFORMS framework to model weekly, daily and hourly series.\autoref{machinelearning} gives background on machine learning interpretability techniques that are used to identify role of features in forecast model selection. In \autoref{results} we discuss the results. \autoref{#conclusions} concludes.

# FFORMS Application to M4 competition data{#fforms}

The FFORMS framework consists of two main components: i) *offline phase*, which includes the development of a classification model and ii) *online phase*, use the classification model developed in the offline phase to identify "best" forecast-model. We develop separate classifiers for yearly, monthly, quarterly, weekly, daily and hourly series. 


## FFORMS framework: offline phase

### observed sample

We split the time series in the M4 competition into training set and test set. The time series in the training set are used as the set of observed time series. The time series in the test set are used to evaluate the classification models. Further, for yearly, quarterly and monthly time series in addition to the time series provided in the M4 competition we used the time series of M1 and M3 competitions. Table \ref{observedsample} summarizes the number of time series in the observed sample and the test set in each frequency category.

\begin{table}[!h]
\centering
\caption{Composition of the time series in the observed sample and the test set}
\label{observedsample}
\begin{tabular}{l|rrr|r}
\multirow{2}{*}{Frequency} & \multicolumn{3}{l|}{Observed Sample} &  Test set \\ 
                  &   M1    &    M3   &    M4  &  M4 \\ \hline
  Yearly          &   181    &   645    &   22000   & 1000 \\
  Quarterly       &   203    &    756   &   23000   &  1000\\
  Monthly         &   617    &    1428   &  47000    &  1000\\
  Weekly          &   -    &   -    &   259   & 100 \\
  Daily           &   -    &   -    &   4001   & 226 \\
  Hourly          &   -    &    -   &  350    & 64\\ \hline
\end{tabular}
\end{table}

### simulated time series

As described in @fforms, we augment the reference set by adding multiple time series simulated based on each series in the M4 competition. We use several standard automatic forecasting algorithms to simulate multiple time series from each series. Table \ref{simulation} shows the different automatic forecasting algorithms used under each frequency category. The automated ETS and ARIMA are implemented using `ets` and `auto.arima` functions available in the forecast package in R [@forecast]. The `stlf` function in the forecast package [@forecast] is used to simulate multiple time series based on multiple seasonal decomposition approach. As shown in Table \ref{simulation} we fit models to each time series in the M4 competition database from the corresponding algorithm and then simulate multiple time series from the selected models. Before simulating time series from daily and hourly series we convert the time series into multiple seasonal time series (msts) objects. For daily time series with length less 366 the frequency is set to 7 and if the time series is long enough to take more than a year (length > 366), the series is converted to a multiple seasonal time series objects with frequencies 7 and 365.25. For hourly series, if the series length is shorter than 168, frequency is set to 24, if the length of the  series is greater than 168 and less than or equals to 8766 only daily and weekly seasonality are allowed setting the frequencies to 24 and 168. In this experiment the length of the simulated time series is set to be equal to: length of the training
period specified in the M4 competition + length of the forecast horizon specified in the competition. For example, the series with id
"Y13190" contains a training period of length 835. The length of the simulated series generated based on this series is equals to 841 (835+6).

\begin{table}[!h]
\centering
\caption{Automatic forecasting algorithms used to simulate time series}
\label{simulation}
\begin{tabular}{lllllll}
 Algorithm & Y & Q & M & W & D &  H \\ \hline
 automated ETS & \checkmark & \checkmark & \checkmark &  &  &  \\
automated ARIMA & \checkmark & \checkmark & \checkmark &  &  &  \\
forecast based on multiple seasonal decomposition &  &  &  & \checkmark & \checkmark & \checkmark\\ \hline
\end{tabular}
\end{table}

As illustrated in @fforms, the observed time series and the simulated time series form the reference to build our classification algorithm. Once we create the reference set for random forest training we split each time series in the reference set into training period and test period. 

### Input: features

The FFORMS framework operates on the features of the time series. For each time series in the reference set features are calculated based on the training period of the time series. 

\begin{table}[!htp]
\centering\footnotesize\tabcolsep=0.12cm
\caption{Time series features}
\label{feature}
\begin{tabular}{llp{8,8cm}cccc}
\toprule
\multicolumn{2}{c}{Feature} & Description & Y & Q/M & W & D/H\\
\midrule
1  & T              & length of time series                                                                   & \yes  & \yes & \yes & \yes\\
2  & trend          & strength of trend                                                                       & \yes  & \yes & \yes & \yes\\
3  & seasonality 1    & strength of seasonality corresponds to frequency 1                                                              & -     & \yes & \yes & \yes\\
4  & seasonality 2    & strength of seasonality corresponds to frequency 2                                                              & -     & - & -& \yes\\
5  & linearity      & linearity                                                                               & \yes  & \yes & \yes & \yes\\
6  & curvature      & curvature                                                                               & \yes  & \yes & \yes & \yes\\
7  & spikiness      & spikiness                                                                               & \yes  & \yes & \yes & \yes\\
8  & e\_acf1        & first ACF value of remainder series                                                     & \yes  & \yes & \yes & \yes\\
9  & stability      & stability                                                                               & \yes  & \yes & \yes & \yes\\
10  & lumpiness      & lumpiness                                                                               & \yes  & \yes & \yes & \yes\\
11 & entropy        & spectral entropy                                                                        & \yes  & \yes & \yes & \yes\\
12 & hurst          & Hurst exponent                                                                          & \yes  & \yes & \yes & \yes\\
13 & nonlinearity   & nonlinearity                                                                            & \yes\ & \yes & \yes & \yes\\
14 & alpha          & ETS(A,A,N) $\hat\alpha$                                                                 & \yes  & \yes & \yes & -\\
15 & beta           & ETS(A,A,N) $\hat\beta$                                                                  & \yes  & \yes & \yes & - \\
16 & hwalpha        & ETS(A,A,A) $\hat\alpha$                                                                 & -     & \yes & - & -\\
17 & hwbeta         & ETS(A,A,A) $\hat\beta$                                                                  & -     & \yes & - & - \\
18 & hwgamma        & ETS(A,A,A) $\hat\gamma$                                                                 & -     & \yes & - &-\\
19 & ur\_pp         & test statistic based on Phillips-Perron test                                            & \yes  & - & - & - \\
20 & ur\_kpss       & test statistic based on KPSS test                                                       & \yes  & - & - & - \\
21 & y\_acf1        & first ACF value of the original series                                                  & \yes  & \yes & \yes & \yes\\
22 & diff1y\_acf1   & first ACF value of the differenced series                                               & \yes  & \yes & \yes & \yes\\
23 & diff2y\_acf1   & first ACF value of the twice-differenced series                                         & \yes  & \yes & \yes & \yes\\
24 & y\_acf5        & sum of squares of first 5 ACF values of original series                                 & \yes  & \yes & \yes & \yes\\
25 & diff1y\_acf5   & sum of squares of first 5 ACF values of differenced series                              & \yes  & \yes & \yes & \yes\\
26 & diff2y\_acf5   & sum of squares of first 5 ACF values of twice-differenced series                        & \yes  & \yes & \yes & \yes \\
27 & seas\_acf1     & autocorrelation coefficient at first seasonal lag                                       & -     & \yes & \yes & \yes\\
28 & sediff\_acf1   & first ACF value of seasonally-differenced series                                        & -     & \yes & \yes & \yes\\
29 & sediff\_seacf1 & ACF value at the first seasonal lag of seasonally-differenced series                    & -     & \yes & \yes & \yes\\
30 & sediff\_acf5   & sum of squares of first 5 autocorrelation coefficients of seasonally-differenced series & -     & \yes & \yes & \yes\\
31 & seas\_pacf     & partial autocorrelation coefficient at first seasonal lag & -     & \yes & \yes & \yes\\
32 & lmres\_acf1    & first ACF value of residual series of linear trend model                                & \yes  & - & - & -\\
33 & y\_pacf5       & sum of squares of first 5 PACF values of original series                                & \yes  & \yes & \yes & \yes\\
34 & diff1y\_pacf5  & sum of squares of first 5 PACF values of differenced series                             & \yes  & \yes & \yes & \yes\\
35 & diff2y\_pacf5  & sum of squares of first 5 PACF values of twice-differenced series                       & \yes  & \yes & \yes & \yes\\
\bottomrule
 \end{tabular}
\end{table}

The description of the features calculated under each frequency category is shown in Table \ref{feature}. A comprehensive description of the features used in the experiment is given in @fforms.

### Output: class-labels

In addition to the class labels used by @fforms we include some more class labels when applying the FFORMS framework to the M4 competition time series. The description of class labels considered under each frequency is shown in Table \ref{classlabels}. We fit the corresponding models outlined in Table \ref{classlabels} to each series in the reference set. The models are estimated using the training period for each series, and forecasts are produced for the test periods. 

\begin{table}[!htp]
\centering\footnotesize\tabcolsep=0.12cm
\caption{Class labels}
\label{classlabels}
\begin{tabular}{llrrrr}
class label & Description & Y & Q/M & W & D/H \\ \hline
WN & white noise process & \checkmark & \checkmark & \checkmark & \checkmark \\
AR/MA/ARMA & AR, MA, ARMA processes & \checkmark & \checkmark & \checkmark & -\\
ARIMA & ARIMA process & \checkmark & \checkmark & \checkmark & - \\
SARIMA & seasonal ARIMA & \checkmark & \checkmark & \checkmark & -\\
RWD & random walk with drift & \checkmark & \checkmark & \checkmark & \checkmark \\
RW & random walk & \checkmark & \checkmark & \checkmark & \checkmark  \\
Theta & standard theta method & \checkmark & \checkmark & \checkmark & \checkmark \\
STL-AR &  & - & \checkmark & \checkmark & \checkmark \\
ETS-notrendnoseasonal & ETS without trend and seasonal components & \checkmark & \checkmark & \checkmark & - \\
ETStrendonly & ETS with trend component and without seasonal component & \checkmark & \checkmark & \checkmark & -\\
ETSdampedtrend & ETS with damped trend component and without seasonal component  & \checkmark &  \checkmark & - & - \\
ETStrendseasonal & ETS with trend and seasonal components & - & \checkmark & - & - \\
ETSdampedtrendseasonal & ETS with damped trend and seasonal components & - & \checkmark & - & -\\
ETSseasonalonly & ETS with seasonal components and without trend component & -  & \checkmark & - & - \\
snaive & seasonal naive method & \checkmark & \checkmark & \checkmark & \checkmark \\
tbats & TBATS forecasting & - & \checkmark & \checkmark & \checkmark \\
nn & neural network time series forecasts & \checkmark & \checkmark & \checkmark & \checkmark \\
mstlets &  & - & - & \checkmark & \checkmark \\
mstlarima & & - & - & - & \checkmark \\\hline
\end{tabular}
\end{table}

The `auto.arima` and `ets` functions in the forecast package are used to identify the suitable (S)ARIMA and ETS models. In order to identify the "best" forecast-model for each time series in the reference set we combine the mean Absolute Scaled Error (MASE) and the symmetric Mean Absolute Percentage Error (MAPE) calculated over the test set. More specifically, for each series both forecast error measures MASE and sMAPE are calculated for each of the forecast models. Each of these is respectively standardized by the median MASE and median sMAPE calculated across the methods. The model with the lowest average value of the scaled MASE and scaled sMAPE is selected as the output class-label. Most of the labels given in Table \ref{classlabels} are self-explanatory labels. In STL-AR, mstlets, and mstlarima, first STL decomposition method applied to the time series and then seasonal naive method is used to forecast the seasonal component. Finally,  AR, ETS and ARIMA models are used to forecast seasonally adjusted data respectively.



### Train a random forest classifier

A random forest with class priors is used to develop the classifier. We build separate random forest classifiers for yearly, quarterly, monthly, weekly, daily and hourly time series. The wrapper function called `build_rf` in the `seer` package enables the training of a random forest and returns class labels("best" forecast-model) for each time series. 

## FFORMS framework: online phase

The online phase of the algorithm involves generating point forecasts and 95% prediction intervals for the M4 competition data. First, the corresponding features are calculated based on the full length of the training period provided by the M4 competition. Second, point forecasts and 95% prediction intervals are calculated based on the predicted class labels, in this case forecast-models. Finally, all negative values are set to zero.


# Machine Learning Interpretability{#machinelearning}

In recent years, there have been a growing interest for interpretability of machine learning algorithms with European General Data Projection Regulation (GDPR) stipulates the explainability of all automatically made decision concerning individuals. We explore the role of features in three different angles: i) global interpretability, and ii) local interpretability. We will introduce each of these ideas briefly below. Model-diagnostics tools are used. 

## General Notation

Let \(\mathcal{P}=\{(\mathbf{x^{(i)}}, y^{(i)})\}_{i=1}^{N}\) be the
historical data set we use to train the classifier. Consider a
p-dimensional feature vector \(X=(X_1, X_2, ..., X_p)\) and a dependent
variable, best forecasting method for each series \(Y\). Let \(\mathcal{F}\) be the unkown relationship between \(X\) and
\(Y\). @Zhao term this as "law of nature". Inside the FFORMS framework, random forest algorithm tries to learn this relationship using
the historical data we provided. We denote the predicted function as
\(\hat{\mathcal{F}}\).

## Global Interpretability Methods

Global interpretability evaluate the behavior of a model on entire data set. Global perspective of model interpretation helps users to understand the overall modeled relationship between features and the model outcome. For example, which features are contribute mostly to the predictive mechanism of the fitted model, complex interactions between features, etc. In the following subsections we provide a description of tools we use to explore the global perspective of the model.  

### Permutation-based variable importance measure

The permutation-based variable importance introduced by @breiman2001random measures the the prediction
strength of each feature. This measure is calculated based on the  out-of-bag (OOB) observations. The calculation of variable importance is formalized as follow: Let $\bar{\mathcal{B}}^{(k)}$ be the OOB sample for a tree $k$, with $k\in \{1,...,ntree\}$, where $ntree$ is the number of trees in the random forest. Then the variable importance of variable $X_{j}$ in $k^{th}$ tree is:
 \[VI^{(k)}(X_{j})=\frac{\sum_{i\in \bar{\mathcal{B}}^{(k)}}I(\gamma_{i}=\gamma_{i,\pi_{j}}^{k})}{|\bar{\mathcal{B}}^{(k)}|}-\frac{\sum_{i\in \bar{\mathcal{B}}^{(k)}}I(\gamma_{i}=\gamma_{i}^{k})}{|\bar{\mathcal{B}}^{(k)}|},\]
 where $\gamma_{i}^{k}$ denotes the predicted class for the $i^{th}$ observation before permuting the values of $X_{j}$ and $\gamma_{i, \pi_{j}}^{k}$ is the predicted class for the $i^{th}$ observation after permuting the values of $X_{j}$. The overall variable importance score is calculated as:
 \[VI(X_{j})=\frac{\sum_{1}^{ntree}VI^{(t)}(x_{j})}{ntree}.\]

Permutation-based variable importance measures provide a useful starting point for identifying relative influence of features on the predicted outcome.  However, they provide a little indication of the nature of the relationship between the features and model outcome. To gain further insights into the role of features inside the FFORMS framework we use partial dependence plot (PDP) introduced by @friedman2008predictive. 

### Partial dependence plot (PDP)

Partial dependence plot can be used to graphically examine how each feature is related to the model prediction while accounting for the average effect of other features in the model. This method is particularly useful to discover complex model structures in "black box" models. Let $X_s$ be the feature we want examine partial dependencies for and $X_c$ be the remaining set of features in $X$.  Then $f_s$, the partial dependence function on $x_s$ is defines as 
\{f_s=E_x}


### Variable importance measure based on PDP

### ICE plots

While partial dependence plots. However, in the presence of substantial interaction between features, the partial dependence response curves can be misleading. 
ICE curves are an extension of PD-plots but, rather than plot the average marginal effect on the response variable, we plot the change in the predicted response variable for each observation as we vary each feature. 


### Variable importance measure based on ICE curves

PD plots, ICE curves and PD-, ICE-associated measures are computationally intensive to create, especially when there are large number of observations in the training set. Hence those measures are calculated based on the subset of randomly selected training examples.

Many of the other techniques exist for visualizing the relationship between the predictors and the response based on a fitted model. For example, 

## Representation of model in the data space (m-in-ds) and data in the model space (d-in-ms)

## Local Interpretability Methods

Global interpretations help us to understand entire modeled relationship.  Local interpretations help us understand the predictions of the model for a single instance or a group of similar instances. Local view of feature contribution provides valuable information about the reliability of FFORMS prediction for a particular instance.

## Model-diagnostics

In addition to the previews discussions xx propose a novel application of 

### Out-of-bag(OOB) error and uncertainty measure for each observation

It is argued in order to estimate the test error of a bagged model it is not necessary to perform cross-validation approach, because each tree is grown using different bootstrap samples from  the training set and a part of training data is not used in the tree construction [ref]. In general, each bagged tree does not make use of around one third of observations to construct the decision tree. These observations are referred to as the out-of-bag(OOB) observations. Each tree is grown based on different bootstrap samples hence, each tree has different set of OOB observations. These OOB samples can be used to calculate internal estimation of the test set error, which is known as OOB error. 

\newpage

# Results{#results}

### Yearly data

**Model diagnostic: FFORMS framework,Yearly series**

```{r yearly_oob, fig.height=10, fig.width=10, fig.cap="Distribution of proportion of times each yearly time series was assigned to each class in the forest.  Each row represent the predicted class label and colour of boxplots corresponds to true class label. There are ten rows in the plot corresponds to each predicted class represented by Y-axis. X-axis denotes the proportion of times a time series is classified in each class. On each row, the true class label match with the predicted class label category dominated the top, indicating a fairly good classification of the model fitted.", fig.pos="h", cache=TRUE}
```

\ref{fig:yearly_oob} shows the distribution of proportion of times each observation (in our case each time series) was assigned to each class based on OOB sample. Each row represent the **predicted class label** and colour of boxplots corresponds to **true class label**. The proportion 1 indicates, that the time series was always predicted to the corresponding class and 0 being never. This is an alternative way of visualizing the vote-matrix information in the random forest model. The other way of representing vote matrix involves ternary plot (@sutherland2000orca) and jittered side-by-side dotplot [@ehrlinger2015ggrandomforests; @da2017interactive]. To over come the problem of overlapping classes arises due to the scale of the training data set, number of classes and similar types of classes of data, boxplot diagrams are used. On each row of \autoref{fig:yearly_oob} the distribution of proportions corresponds to the time series in which the predicted class label and true class label are the same dominates the top indicating a fairly good classification of the model. In addition to that, in each row the distributions corresponds to the time series labeled similar to the predicted class label also dominate the others. For example, within ETS-trend predicted class, the distributions correspond to the true class labels, ETS-damped trend, ARIMA, were also assigned with high probability and less values were assigned to ARMA/AR/MA, White noise process and ETS (ANN)/ETS(MNN). This confirms that our FFORMS framework successfully learnt the similarities and dissimilarities between the classes itself. (Each series is classified as random walk in some instances, yearly time series has high chance of getting classified as random walk). One reason for this is as shown in @kang2018efficient yearly series of M1, M3 and M4 are generally trended. (Results of M3 and M4 competition) This diagram helps to evaluate the model performance in the data space (model-in-the-data-space) (@da2017interactive).

**Feature importance and main effects**

Permutation-based variable importance and Gini feature importance measure are used to evaluate the overall feature importance. Moreover, most important features for each class is identified based on three measures: i) permutation-based variable importance, ii) partial dependence functions based variable importance and iii) ICE-curves based variable importance measure. The one that shows the highest importance is ranked 25, the second best is ranked 24, and so on. Finally, for each category, an average rank for each feature is computed based on the mean value of all rankings across all the feature importance metrics considered. The corresponding results are shown in \autoref{fig:vi_yearly}. The features strength of trend and test statistic of Phillips–Perron(PP) unit root test, linearity, first autocorrelation coefficient of the differenced series are appear to be most important features in each class. On the otherhand, sum of squares of first five autocorrelation coefficients of the twice-difference series and lumpiness show lowest contribution across many classes. The length of time series (N) is assigned a high importance in neural-network class compared to others. This could be due to neural network approach may be beneficial in forecasting time series with long history of observations. Further, first correlation coefficient of the twice-differenced series is appear to be most important in ARIMA class as this category contains the higher order differenced series. Hurst exponent and entropy appear to be equally important in stationary classes. Within ETS-damped trend category beta and curvature ranked as important features. 

```{r vi_yearly, fig.height=10, fig.width=10, fig.cap="Feature importance plot yearly series. Longer bars indicate more important features. Top 5 features are highlighted in red.", fig.pos="h"}
```

**Partial dependency curves**

Partial dependency, and associated confidence intervals corresponds to top-three features of each category are plotted to explore the relationship between features and predicted outcome. Confidence intervals also facilitate the indication of interaction effects. Top three features of each category shows a non-linear relationship with predicted outcome. ETS (AAN, MAN, ANN, MNN), ARIMA, ARMA/AR/MA, white noise process and neural network classes show a monotonically increasing or decreasing relationship with trend while theta class has a parabolic relationship with trend. First correlation of the residual series of linear model shows a monotonically increasing relationship with ETS(ANN, MNN) class, whereas the random walk with drift shows an opposite relationship.

**Two-way interaction between features**

The relative strength of two-way interaction between features were determined using formulae developed by @friedman2008predictive, which is implemented in the iml(@molnar2018iml,) package in R. xx shows the heat maps of relative strength of all possible pairwise interactions for each class. The test statistic of Phillip-Perron test, strength of trend, and linearity show a weak interaction  with other features in all the class. These two features are appear to be among top 5 in all the classes according to the variable importance measures. Further, narrow confidence bands corresponds to these features in the partial dependency plots also confirms the less interactivity. In almost all the cases partial correlation and auto-correlation based features are heavily interacting. However, the first correlation coefficient of the difference series do not interact with other features heavily in the case of ARIMA  class. Further, almost all the feature pairs appear to be interacting in deciding probability of assigning to neural network class. 

\newpage

```{r pdp_yearly, fig.height=14.7, fig.width=10, fig.cap="Partial dependence plots for the top ranked features from variable importance measures. The shading shows the 95% confidence intervals. Y-axis denotes the probability of belong to corresponding class.", fig.pos="h"}
```

\newpage

```{r friedmany, fig.height=30, fig.width=20, fig.cap="Heat maps of relative strength of all possible pairwise interactions calclated based on Friedman's H-statistic", fig.pos="h", message=FALSE, warning=FALSE}
```

\newpage


```{r yearly_lime, fig.height=10, fig.width=20, fig.cap="", fig.pos="h", fig.align="center"}
```



### LIME

xx shows the feature contribution for the instances highlighted in xx. 

\newpage
### Quarterly data

In the row categories snaive, stlar tabts, white noise process and neural network classes, the distribution corresponds to the true class label matches with the label of row dominates other. Within snaive, stlar, tbats and white noise classes ETS-seasonal, SARIMA, ETS-trend and ARMA/AR/MA slightly dominates respectively. It could be due to the high-similarity to one another. Further, withing the neural network category ETS (ANN/MNN), ARMA/AR/MA and white noise processes dominates equally. This indicates neural network models share similar type of features with those three classes. Furthermore, except the time series labeled as a ETS models with seasonal component or SARIMA models all other time series have a high probability of being in random walk with drift class irrespective of their true class labels. These results are consisted with the random walk model fitted to yearly data. Except the time series labeled as ARMA/AR/MA all other quarterly time series have a very low chance of being to ARMA/AR/MA class. Further, all distributions corresponds to the tbats row located further away from zero. This indicates all time series are being classified as tats model more than once in the forest. Except few outliars,  distributions within neural network category also show a slight upward deviation from zero. However, the upper boundry of these distributions do not surpass the upper boundaries of dominating box plots in the random walk with drift class and SARIMA class. These types of diversity in the distributions indicates the  the appropriateness of using combination forecasting.  Further, these information are useful in identifying potential time series models for combination forecast and improve the existing combination approaches proposed in the M4-competition. 

**Variable importance: quarterly data**

Strength of seasonality appear to be the most important feature across all categories.
Similar to the results of yearly time series trend and linearity also listed among the top five features in each category. In the case of yearly data low variable importance is assigned to both stability and length of the series, however with quarterly time series within most categories stability and length of the series are assigned a high importance. In additiontion to the strength of seasonality, models with seasonal components assigned high importance to additional features related to seasonality such as ACF or PACF feature related to seasonal lag or seasonally-differenced series, for example, snaive and ETS-seasonal with  partial autocorrelation coefficient at first seasonal lag, etc. Furthurmore, as expected features related to parameter estimates ETS (A, A, A) were selected as important features by ETS with damped trend and seasonal component and ETS with trend and seasonal component models.

\newpage

```{r oob_quarterly, fig.height=28, fig.width=20, fig.cap="Distribution of proportion of times each quarterly time series was assigned to each class in the forest.  Each row represent the predicted class label and colour of boxplots corresponds to true class label. X-axis denotes the proportion of times a time series is classified in each class.", fig.pos="h"}
```

\newpage

```{r vi_quarterly, fig.height=10, fig.width=20, fig.cap="", fig.pos="h", fig.align="center"}
```

\autoref{fig:pdp_quarterly1} shows the partial dependency functions for top three features with in each category. Sesonality shows a linear relationship with probability of being to snaive class. Random walk, random walk with drift, all ETS models without seasonal component, ARIMA and white noise process show a similar pattern of relationship with seasonality with vary degree of width of confidence band while all ETS models with seasonal components and SARIMA hold the opposite patter. SARIMA and ARIMA classes shows a non-monotonic relationship with seasonality. Neural network class shows a non-linear relationship with seasonality, and wide confidence bands indicated sesonality interact with other features when deciding probability of being into neural network class. According to the partial dependency functions corresponds to the rows ETS-NTNS, ETS-DT, ETS-T and ETS-TS confirms how well the model's understanding of feature behaviour matches the domain's expert knowledge which gives rise to the trustability of the FFORMS famework. For example, highly trended and low seasonality time series have a high chance of being classified to ETS-Trend class while  the opposite relationship can be seen in ETS-seasonal category with low trended and highly sesoanly oscillated time series have a high chance of being classified to ETS-seasonal. Eventhough, a different portfolio of time series are used to build the classifier patial dependency functions show a consistency between the results of yearly framework and quarterly framework. For example, partial dependency function of linearity within ARMA/AR/MA class, partial dependency functions corresponds to trend, etc. However, partial dependency functions for quarterly data display much wider confidence bands. This may be due to more interaction between class as we have 17 class labels. xx shows Heat maps of relative strength of all possible pairwise interactions calclated based on Friedman’s H-statistic.

\newpage

```{r pdp_quarterly1, fig.height=45, fig.width=35, fig.cap="Partial dependence plots for top three features in each category (Quarterly)", fig.pos="h"}
```

\newpage

```{r pdp_quarterly2, fig.height=30, fig.width=20, fig.cap="Partial dependence plots for top three features in each category (Quarterly)", fig.pos="h"}
```

\newpage

```{r friedmanQ, fig.height=30, fig.width=20, fig.cap="Heat maps of relative strength of all possible pairwise interactions calclated based on Friedman's H-statistic for quarterly data.", fig.pos="h", message=FALSE, warning=FALSE}
```

\newpage

```{r friedmanQ2, fig.height=10, fig.width=30, fig.cap="Heat maps of relative strength of all possible pairwise interactions calclated based on Friedman's H-statistic for quarterly data(cont).", fig.pos="h", message=FALSE, warning=FALSE}
```



\newpage
## Monthly

```{r oob_monthly, fig.height=30, fig.width=20, fig.cap="", fig.pos="h"}
```

\newpage

```{r vi_monthly, fig.height=10, fig.width=40, fig.cap="", fig.pos="h", fig.align="center"}
```

\autoref(fig: oob_monthly) can be interpreted similar to the results of quarterly data.
For quarterly and monthly data same set of features and class-labels are used in training the model. Hence, this consistency between the results of quarterly and monthly series would provide evidence in support of the validity and trustability of the model.  Seasonality, trend, spikiness and linearity appear to be most important features across all categories. Further, fetaures calculated based on parameter estimates of ETS(A,A,A) and ACF and PACF-based features related to sesoanl lags and seasonally differenced series were assigned higher importance than the quarterly series FFOMS framewor.  One notable difference between quarterly series and monthly series is 


\newpage

```{r pdp_monthly1, fig.height=45, fig.width=35, fig.cap="Partial dependence plots for top three features in each category (Monthly)", fig.pos="h"}
```

\newpage

```{r pdp_monthly2, fig.height=30, fig.width=20, fig.cap="Partial dependence plots for top three features in each category (Monthly)", fig.pos="h"}
```

\newpage

```{r friedmanM, fig.height=30, fig.width=20, fig.cap="Heat maps of relative strength of all possible pairwise interactions calclated based on Friedman's H-statistic for monthly data.", fig.pos="h", message=FALSE, warning=FALSE}
```

\newpage

```{r friedmanM2, fig.height=30, fig.width=20, fig.cap="Heat maps of relative strength of all possible pairwise interactions calclated based on Friedman's H-statistic for monthly data.", fig.pos="h", message=FALSE, warning=FALSE}
```

\newpage

```{r monthly_pca, fig.height=20, fig.width=20, fig.cap="PCA(Monthly)", fig.pos="h"}
```

\newpage
## Weekly

```{r oob_weekly, fig.height=10, fig.width=10, fig.cap="Distribution of proportion of times each weekly time series was assigned to each class in the forest.  Each row represent the predicted class label and colour of boxplots corresponds to true class label. There are ten rows in the plot corresponds to each predicted class represented by Y-axis. X-axis denotes the proportion of times a time series is classified in each class. On each row, the true class label match with the predicted class label category dominated the top, indicating a fairly good classification of the model fitted.", fig.pos="h", cache=TRUE}
```

\newpage

```{r vi_weekly, fig.height=15, fig.width=20, fig.cap="", fig.pos="h", fig.align="center"}
```

\newpage

\newpage

```{r weekly_pdp, fig.height=14.7, fig.width=10, fig.cap="Partial dependence plots for the top ranked features from variable importance measures(weekly series). The shading shows the 95% confidence intervals. Y-axis denotes the probability of belong to corresponding class.", fig.pos="h"}
```

\newpage

```{r weekly_pdp2, fig.height=5, fig.width=10, fig.cap="Partial dependence plots for the top ranked features from variable importance measures(weekly series). The shading shows the 95% confidence intervals. Y-axis denotes the probability of belong to corresponding class(cont.).", fig.pos="h"}
```

\newpage

```{r friedmanHW, fig.height=30, fig.width=20, fig.cap="Heat maps of relative strength of all possible pairwise interactions calclated based on Friedman's H-statistic for weekly data.", fig.pos="h", message=FALSE, warning=FALSE}

```


\newpage

```{r pca_weekly, fig.height=15, fig.width=20, fig.cap="", fig.pos="h", fig.align="center"}
```

\newpage

## Daily

```{r oob_daily, fig.height=10, fig.width=10, fig.cap="Distribution of proportion of times each weekly time series was assigned to each class in the forest.  Each row represent the predicted class label and colour of boxplots corresponds to true class label. There are ten rows in the plot corresponds to each predicted class represented by Y-axis. X-axis denotes the proportion of times a time series is classified in each class. On each row, the true class label match with the predicted class label category dominated the top, indicating a fairly good classification of the model fitted.", fig.pos="h", cache=TRUE}
```

\newpage

```{r vi_daily, fig.height=15, fig.width=20, fig.cap="", fig.pos="h", fig.align="center"}
```

\newpage

```{r daily_pdp, fig.height=14.7, fig.width=10, fig.cap="Partial dependence plots for the top ranked features from variable importance measures(daily series). The shading shows the 95% confidence intervals. Y-axis denotes the probability of belong to corresponding class.", fig.pos="h"}
```

\newpage

```{r pca_daily, fig.height=15, fig.width=20, fig.cap="", fig.pos="h", fig.align="center"}
```

\newpage

## Hourly

```{r hourly_oob, fig.height=30, fig.width=20, fig.cap="", fig.pos="h"}
```

\newpage

```{r vi_hourly, fig.height=10, fig.width=10, fig.cap="Feature importance plot yearly series. Longer bars indicate more important features. Top 5 features are highlighted in red.", fig.pos="h"}
```

\newpage

```{r hourly_pdp, fig.height=14.7, fig.width=10, fig.cap="Partial dependence plots for the top ranked features from variable importance measures(hourly series). The shading shows the 95% confidence intervals. Y-axis denotes the probability of belong to corresponding class.", fig.pos="h"}
```

\newpage

```{r pca_hourly, fig.height=15, fig.width=20, fig.cap="", fig.pos="h", fig.align="center"}
```

# Discussion and Conclusions{#conclusions}

Since several number of features are used to build the framework with comparable contributions, and thus, all individual contributions are small.However, patial dependency plots confirms these individual feature effects captures the expected principles in the field of time series forecasting. 

our proposed method selects a specific feature subset for each class. 
Feature-based time series analysis should develop support systems that incorporate these features whenever feasible and appropriate. We also demonstrate how feature contributions can be applied to understand the dependencies between time series features and their predicted classification and to assess the reliability of prediction. Feature contribution provide valuable information about the reliability of FFORMS prediction for a particular instance. Application of feature contributions for model interpretation is particularly valuable for ongoing research in  the field of feature-based time series analysis. We argue this analysis of provides a more refine 
picture of time series features and forecast model selection.

# Appendix {-}

\newpage
## PCA space{-}

```{r yearly_pca, fig.height=10, fig.width=20, fig.cap="", fig.pos="h", fig.align="center"}
```


```{r quarterly_pca, fig.height=20, fig.width=20, fig.cap="PCA(Quarterly)", fig.pos="h"}
```


# References
