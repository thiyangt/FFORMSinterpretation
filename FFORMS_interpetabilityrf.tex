\documentclass[11pt,a4paper,]{article}
\usepackage{lmodern}

\usepackage{amssymb,amsmath}
\usepackage{ifxetex,ifluatex}
\usepackage{fixltx2e} % provides \textsubscript
\ifnum 0\ifxetex 1\fi\ifluatex 1\fi=0 % if pdftex
  \usepackage[T1]{fontenc}
  \usepackage[utf8]{inputenc}
\else % if luatex or xelatex
  \usepackage{unicode-math}
  \defaultfontfeatures{Ligatures=TeX,Scale=MatchLowercase}
\fi
% use upquote if available, for straight quotes in verbatim environments
\IfFileExists{upquote.sty}{\usepackage{upquote}}{}
% use microtype if available
\IfFileExists{microtype.sty}{%
\usepackage[]{microtype}
\UseMicrotypeSet[protrusion]{basicmath} % disable protrusion for tt fonts
}{}
\PassOptionsToPackage{hyphens}{url} % url is loaded by hyperref
\usepackage[unicode=true]{hyperref}
\hypersetup{
            pdftitle={Peeking inside FFORMS: Feature-based FORecast Model Selection},
            pdfkeywords={FFORMS, time series, machine learning interpretability, black-box
models, LIME},
            pdfborder={0 0 0},
            breaklinks=true}
\urlstyle{same}  % don't use monospace font for urls
\usepackage{geometry}
\geometry{left=2.5cm,right=2.5cm,top=2.5cm,bottom=2.5cm}
\usepackage[style=authoryear-comp,]{biblatex}
\addbibresource{references.bib}
\usepackage{longtable,booktabs}
% Fix footnotes in tables (requires footnote package)
\IfFileExists{footnote.sty}{\usepackage{footnote}\makesavenoteenv{long table}}{}
\usepackage{graphicx,grffile}
\makeatletter
\def\maxwidth{\ifdim\Gin@nat@width>\linewidth\linewidth\else\Gin@nat@width\fi}
\def\maxheight{\ifdim\Gin@nat@height>\textheight\textheight\else\Gin@nat@height\fi}
\makeatother
% Scale images if necessary, so that they will not overflow the page
% margins by default, and it is still possible to overwrite the defaults
% using explicit options in \includegraphics[width, height, ...]{}
\setkeys{Gin}{width=\maxwidth,height=\maxheight,keepaspectratio}
\IfFileExists{parskip.sty}{%
\usepackage{parskip}
}{% else
\setlength{\parindent}{0pt}
\setlength{\parskip}{6pt plus 2pt minus 1pt}
}
\setlength{\emergencystretch}{3em}  % prevent overfull lines
\providecommand{\tightlist}{%
  \setlength{\itemsep}{0pt}\setlength{\parskip}{0pt}}
\setcounter{secnumdepth}{5}

% set default figure placement to htbp
\makeatletter
\def\fps@figure{htbp}
\makeatother


\title{Peeking inside FFORMS: Feature-based FORecast Model Selection}

%% MONASH STUFF

%% CAPTIONS
\RequirePackage{caption}
\DeclareCaptionStyle{italic}[justification=centering]
 {labelfont={bf},textfont={it},labelsep=colon}
\captionsetup[figure]{style=italic,format=hang,singlelinecheck=true}
\captionsetup[table]{style=italic,format=hang,singlelinecheck=true}

%% FONT
\RequirePackage{bera}
\RequirePackage{mathpazo}

%% HEADERS AND FOOTERS
\RequirePackage{fancyhdr}
\pagestyle{fancy}
\rfoot{\Large\sffamily\raisebox{-0.1cm}{\textbf{\thepage}}}
\makeatletter
\lhead{\textsf{\expandafter{\@title}}}
\makeatother
\rhead{}
\cfoot{}
\setlength{\headheight}{15pt}
\renewcommand{\headrulewidth}{0.4pt}
\renewcommand{\footrulewidth}{0.4pt}
\fancypagestyle{plain}{%
\fancyhf{} % clear all header and footer fields
\fancyfoot[C]{\sffamily\thepage} % except the center
\renewcommand{\headrulewidth}{0pt}
\renewcommand{\footrulewidth}{0pt}}

%% MATHS
\RequirePackage{bm,amsmath}
\allowdisplaybreaks

%% GRAPHICS
\RequirePackage{graphicx}
\setcounter{topnumber}{2}
\setcounter{bottomnumber}{2}
\setcounter{totalnumber}{4}
\renewcommand{\topfraction}{0.85}
\renewcommand{\bottomfraction}{0.85}
\renewcommand{\textfraction}{0.15}
\renewcommand{\floatpagefraction}{0.8}

%\RequirePackage[section]{placeins}

%% SECTION TITLES
\RequirePackage[compact,sf,bf]{titlesec}
\titleformat{\section}[block]
  {\fontsize{15}{17}\bfseries\sffamily}
  {\thesection}
  {0.4em}{}
\titleformat{\subsection}[block]
  {\fontsize{12}{14}\bfseries\sffamily}
  {\thesubsection}
  {0.4em}{}
\titlespacing{\section}{0pt}{*5}{*1}
\titlespacing{\section}{0pt}{*2}{*0.2}


%% TITLE PAGE
\def\Date{\number\day}
\def\Month{\ifcase\month\or
 January\or February\or March\or April\or May\or June\or
 July\or August\or September\or October\or November\or December\fi}
\def\Year{\number\year}

\makeatletter
\def\wp#1{\gdef\@wp{#1}}\def\@wp{??/??}
\def\jel#1{\gdef\@jel{#1}}\def\@jel{??}
\def\showjel{{\large\textsf{\textbf{JEL classification:}}~\@jel}}
\def\nojel{\def\showjel{}}
\def\addresses#1{\gdef\@addresses{#1}}\def\@addresses{??}
\def\cover{{\sffamily\setcounter{page}{0}
        \thispagestyle{empty}%
        \vspace*{-2cm}
        \centerline{\raisebox{-1.8cm}{\includegraphics[width=5cm]{MBSportrait}}\hspace*{9cm} ISSN 1440-771X}\vspace{0.99cm}
        \begin{center}\Large
        Department of Econometrics and Business Statistics\\[.5cm]
        \scriptsize http://business.monash.edu/econometrics-and-business-statistics/research/publications
        \end{center}\vspace{2cm}
        \begin{center}
        \fbox{\parbox{14cm}{\begin{onehalfspace}\centering\Huge\vspace*{0.3cm}
                \textsf{\textbf{\expandafter{\@title}}}\vspace{1cm}\par
                \LARGE\@author\end{onehalfspace}
        }}
        \end{center}
        \vfill
                \begin{center}\Large
                \Month~\Year\\[1cm]
                Working Paper \@wp
        \end{center}}}
\def\pageone{{\sffamily\setstretch{1}%
        \thispagestyle{empty}%
        \vbox to \textheight{%
        \raggedright\baselineskip=1.2cm
     {\fontsize{24.88}{30}\sffamily\textbf{\expandafter{\@title}}}
        \vspace{2cm}\par
        \hspace{1cm}\parbox{14cm}{\sffamily\large\@addresses}\vspace{1cm}\vfill
        \hspace{1cm}{\large\Date~\Month~\Year}\\[1cm]
        \hspace{1cm}\showjel\vss}}}
\def\blindtitle{{\sffamily
     \thispagestyle{plain}\raggedright\baselineskip=1.2cm
     {\fontsize{24.88}{30}\sffamily\textbf{\expandafter{\@title}}}\vspace{1cm}\par
        }}
\def\titlepage{{\cover\newpage\pageone\newpage\blindtitle}}

\def\blind{\def\titlepage{{\blindtitle}}\let\maketitle\blindtitle}
\def\titlepageonly{\def\titlepage{{\pageone\end{document}}}}
\def\nocover{\def\titlepage{{\pageone\newpage\blindtitle}}\let\maketitle\titlepage}
\let\maketitle\titlepage
\makeatother

%% SPACING
\RequirePackage{setspace}
\spacing{1.5}

%% LINE AND PAGE BREAKING
\sloppy
\clubpenalty = 10000
\widowpenalty = 10000
\brokenpenalty = 10000
\RequirePackage{microtype}

%% PARAGRAPH BREAKS
\setlength{\parskip}{1.4ex}
\setlength{\parindent}{0em}

%% HYPERLINKS
\RequirePackage{xcolor} % Needed for links
\definecolor{darkblue}{rgb}{0,0,.6}
\RequirePackage{url}

\makeatletter
\@ifpackageloaded{hyperref}{}{\RequirePackage{hyperref}}
\makeatother
\hypersetup{
     citecolor=0 0 0,
     breaklinks=true,
     bookmarksopen=true,
     bookmarksnumbered=true,
     linkcolor=darkblue,
     urlcolor=blue,
     citecolor=darkblue,
     colorlinks=true}

%% KEYWORDS
\newenvironment{keywords}{\par\vspace{0.5cm}\noindent{\sffamily\textbf{Keywords:}}}{\vspace{0.25cm}\par\hrule\vspace{0.5cm}\par}

%% ABSTRACT
\renewenvironment{abstract}{\begin{minipage}{\textwidth}\parskip=1.4ex\noindent
\hrule\vspace{0.1cm}\par{\sffamily\textbf{\abstractname}}\newline}
  {\end{minipage}}


\usepackage[T1]{fontenc}
\usepackage[utf8]{inputenc}

\usepackage[showonlyrefs]{mathtools}
\usepackage[no-weekday]{eukdate}

%% BIBLIOGRAPHY

\makeatletter
\@ifpackageloaded{biblatex}{}{\usepackage[style=authoryear-comp, backend=biber, natbib=true]{biblatex}}
\makeatother
\ExecuteBibliographyOptions{bibencoding=utf8,minnames=1,maxnames=3, maxbibnames=99,dashed=false,terseinits=true,giveninits=true,uniquename=false,uniquelist=false,doi=false, isbn=false,url=true,sortcites=false}

\DeclareFieldFormat{url}{\texttt{\url{#1}}}
\DeclareFieldFormat[article]{pages}{#1}
\DeclareFieldFormat[inproceedings]{pages}{\lowercase{pp.}#1}
\DeclareFieldFormat[incollection]{pages}{\lowercase{pp.}#1}
\DeclareFieldFormat[article]{volume}{\mkbibbold{#1}}
\DeclareFieldFormat[article]{number}{\mkbibparens{#1}}
\DeclareFieldFormat[article]{title}{\MakeCapital{#1}}
\DeclareFieldFormat[article]{url}{}
%\DeclareFieldFormat[book]{url}{}
%\DeclareFieldFormat[inbook]{url}{}
%\DeclareFieldFormat[incollection]{url}{}
%\DeclareFieldFormat[inproceedings]{url}{}
\DeclareFieldFormat[inproceedings]{title}{#1}
\DeclareFieldFormat{shorthandwidth}{#1}
%\DeclareFieldFormat{extrayear}{}
% No dot before number of articles
\usepackage{xpatch}
\xpatchbibmacro{volume+number+eid}{\setunit*{\adddot}}{}{}{}
% Remove In: for an article.
\renewbibmacro{in:}{%
  \ifentrytype{article}{}{%
  \printtext{\bibstring{in}\intitlepunct}}}

\AtEveryBibitem{\clearfield{month}}
\AtEveryCitekey{\clearfield{month}}

\makeatletter
\DeclareDelimFormat[cbx@textcite]{nameyeardelim}{\addspace}
\makeatother
\renewcommand*{\finalnamedelim}{%
  %\ifnumgreater{\value{liststop}}{2}{\finalandcomma}{}% there really should be no funny Oxford comma business here
  \addspace\&\space}


\wp{no/yr}
\jel{C10,C14,C22}


\blind



\date{\sf\Date~\Month~\Year}
\makeatletter
 \lfoot{\sf\@date}
\makeatother

%% Any special functions or other packages can be loaded here.
%% Any special functions or other packages can be loaded here.
\usepackage{tikz}
\usepackage{algorithm}
\usepackage{algpseudocode}
\usepackage{amsthm}
\usepackage{amsmath,bm}
\usepackage{paralist}
\usepackage{todonotes}
\usepackage{ctable}
\usepackage{multirow}
\usepackage{lscape}
\usepackage{rotating}
\usepackage{float} 
\floatplacement{figure}{H} 

\def\sectionautorefname{Section}
\captionsetup[figure]{font=small}
\captionsetup[table]{font=small}
\def\var{\text{Var}}
\allowdisplaybreaks
\sloppy

%% LINE AND PAGE BREAKING
\clubpenalty = 4500
\widowpenalty = 4500
\brokenpenalty = 4500


\def\yes{$\checkmark$}

\setlength{\abovedisplayskip}{5pt}
\setlength{\belowdisplayskip}{5pt}
\setlength{\abovedisplayshortskip}{0pt}
\setlength{\belowdisplayshortskip}{0pt}


\usepackage{amsthm}
\newtheorem{theorem}{Theorem}[section]
\newtheorem{lemma}{Lemma}[section]
\theoremstyle{definition}
\newtheorem{definition}{Definition}[section]
\newtheorem{corollary}{Corollary}[section]
\newtheorem{proposition}{Proposition}[section]
\theoremstyle{definition}
\newtheorem{example}{Example}[section]
\theoremstyle{definition}
\newtheorem{exercise}{Exercise}[section]
\theoremstyle{remark}
\newtheorem*{remark}{Remark}
\newtheorem*{solution}{Solution}
\begin{document}
\maketitle
\begin{abstract}
This study investigates the relationship between features of time series
and the choice of forecast-model selection using FFORMS framework.
FFORMS framework build a mapping that relates the features of time
series to the ``best'' forecast-model using random forest algorithm. In
this paper we make a novel use of FFORMS framework to explore the nature
of the role of features to the predictive mechanism. We explore the role
of features in two different perspectives: i) Global contribution of
features: overall role of features in the choice of different forecast
model selection and ii) Local contribution of features: what features
are most influential for the prediction of a specific time series?. This
is accomplished by using model-agnostic machine learning
interpretability approaches. Partial-dependency plots are used to
visualize both main and interaction effects of features. The results of
this study provide a more refine picture of the relationship between
features and the choice of forecast-model which is particularly valuable
for ongoing research in the field of feature-based time series analysis.
\end{abstract}
\begin{keywords}
FFORMS, time series, machine learning interpretability, black-box
models, LIME
\end{keywords}

\section{Introduction}\label{intro}

The time series forecasting field has been evolving for a long time and
has introduced a wide variety of forecasting methods. However, for a
given time series the selection of an appropriate forecasting-method
among many possibilities is not straight forward. This selection is one
of the most difficult tasks as each method perform best for some but not
all tasks. The features of time series are considered to be an important
factor in identifying suitable forecasting models
\autocites{meade2000evidence}{makridakis2000m3}. However, a description
of relationship between the features and the performance of algorithms
is rarely discussed in the field of forecasting.

There have been several recent studies on use of supervised learning
algorithms to automate the forecast model selection based on the
features computed from the time series
\autocites{shah1997model}{prudencio2004meta}{lemke2010meta}{kuck2016meta}.
Meta-learning approach provides a systematic guidance on model selection
based on knowledge acquire from historical data set, in our case
historical collection of time series. The key idea behind these
framework is, forecast-model selection is posed as a supervised learning
task. Each time series in the meta-data set is represented as a vector
of features and labeled according to the ``best'' forecasting
method(i.e.~lowest MASE, etc.). Then a meta-learner is trained to
identify suitable forecasting models. With the era of big data, such an
automated model selection process is necessary because the cost of
invoking all possible forecasting method is prohibitive. However, these
work suffer from the limitation of providing meaningful interpretations
that can enhance understanding of relations between features and model
outcomes. To best of our knowledge, very limited efforts have been taken
to understand how the models are making its decisions and what is really
happening inside these complex model structures. This results in less
transparency of the model which lead to the questions of

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  How features are related to the property being modeled?
\item
  How features interact with each other to identify the suitable
  forecasting method?
\item
  Why certain features were responsible in driving certain decisions?
\item
  Which features contribute the most to classify a specific instance?
\end{enumerate}

On the other hand, aside from the goal of developing automated
forecast-model selection framework few researchers have made an attempt
to provide a description of relationship between the features and the
performance of algorithms
\autocites{schnaars1984situational}{wang2009rule}{lemke2010meta}{petropoulos2014horses}.
However, these studies are limited by the scale of problem instances
used, diversity of forecasting-methods used, quality of features
considered, and modelling approached used to identify the relationship
between features and forecast model performance. Most of these studies
are typically restricted to simple statistical techniques such as simple
linear models, decision trees, etc.

To fill this gap, this paper makes a first step towards providing a
comprehensive explanation of the relationship between time series
features and forecast-model selection using machine learning
interpretability techniques. This paper builds on the method from our
previous work ref, in which we introduced the FFORMS (Feature-based
FORecast Model Selection) framework. The random forest algorithm is used
to model the relationship between features and ``best'' performing
forecast-model. A large collection of time series is used to train the
model. We use 35 features to capture different characteristics of time
series. The reason for the choice of random forest algorithm is its
ability to model complex variable interactions. One noticeable
significance of our approach is this can be parallized to for any given
computing budget and time. Although the prediction accuracy of random
forest algorithm is high, it is not easy to interpret what is happening
inside the forest because of the two-step randomization. In this work we
aim at providing a deeper understanding of the underlying mechanism and
influence of features in forecast-model selection.

{[}What is the usefulness of analyzing the relationship between features
and model performance?{]} Understanding the role of features is
worthwhile even if producing an accurate and generalizable model is the
only objective of the modelling. This is because the less transparency
of the model may be distrusted regardless of their predictive
performance. The methodology we propose here is a novel application of
machine learning interpretability methods to visualize and explore the
role of features in forecast-model selection.

This paper proceeds as follows. In \autoref{fforms} we describe the
application of FFORMS framework to M4competition data. The main
contribution of this from our previous work \textcite{fforms} is we
extend the FFORMS framework to model weekly, daily and hourly series.
\autoref{machinelearning} gives background on machine learning
interpretability techniques that are used to identify role of features
in forecast model selection. In \autoref{results} we discuss the
results. \autoref{conclusions} concludes.

\section{FFORMS Application to M4 competition data}\label{fforms}

The FFORMS framework consists of two main components: i) \emph{offline
phase}, which includes the development of a classification model and ii)
\emph{online phase}, use the classification model developed in the
offline phase to identify ``best'' forecast-model. We develop separate
classifiers for yearly, monthly, quarterly, weekly, daily and hourly
series.

\subsection{FFORMS framework: offline
phase}\label{fforms-framework-offline-phase}

\subsubsection{observed sample}\label{observed-sample}

We split the time series in the M4 competition into training set and
test set. The time series in the training set are used as the set of
observed time series. The time series in the test set are used to
evaluate the classification models. Further, for yearly, quarterly and
monthly time series in addition to the time series provided in the M4
competition we used the time series of M1 and M3 competitions. Table
\ref{observedsample} summarizes the number of time series in the
observed sample and the test set in each frequency category.

\begin{table}[!h]
\centering
\caption{Composition of the time series in the observed sample and the test set}
\label{observedsample}
\begin{tabular}{l|rrr|r}
\multirow{2}{*}{Frequency} & \multicolumn{3}{l|}{Observed Sample} &  Test set \\ 
                  &   M1    &    M3   &    M4  &  M4 \\ \hline
  Yearly          &   181    &   645    &   22000   & 1000 \\
  Quarterly       &   203    &    756   &   23000   &  1000\\
  Monthly         &   617    &    1428   &  47000    &  1000\\
  Weekly          &   -    &   -    &   259   & 100 \\
  Daily           &   -    &   -    &   4001   & 226 \\
  Hourly          &   -    &    -   &  350    & 64\\ \hline
\end{tabular}
\end{table}

\subsubsection{simulated time series}\label{simulated-time-series}

As described in \textcite{fforms}, we augment the reference set by
adding multiple time series simulated based on each series in the M4
competition. We use several standard automatic forecasting algorithms to
simulate multiple time series from each series. Table \ref{simulation}
shows the different automatic forecasting algorithms used under each
frequency category. The automated ETS and ARIMA are implemented using
\texttt{ets} and \texttt{auto.arima} functions available in the forecast
package in R \autocite{forecast}. The \texttt{stlf} function in the
forecast package \autocite{forecast} is used to simulate multiple time
series based on multiple seasonal decomposition approach. As shown in
Table \ref{simulation} we fit models to each time series in the M4
competition database from the corresponding algorithm and then simulate
multiple time series from the selected models. Before simulating time
series from daily and hourly series we convert the time series into
multiple seasonal time series (msts) objects. For daily time series with
length less 366 the frequency is set to 7 and if the time series is long
enough to take more than a year (length \textgreater{} 366), the series
is converted to a multiple seasonal time series objects with frequencies
7 and 365.25. For hourly series, if the series length is shorter than
168, frequency is set to 24, if the length of the series is greater than
168 and less than or equals to 8766 only daily and weekly seasonality
are allowed setting the frequencies to 24 and 168. In this experiment
the length of the simulated time series is set to be equal to: length of
the training period specified in the M4 competition + length of the
forecast horizon specified in the competition. For example, the series
with id ``Y13190'' contains a training period of length 835. The length
of the simulated series generated based on this series is equals to 841
(835+6).

\begin{table}[!h]
\centering
\caption{Automatic forecasting algorithms used to simulate time series}
\label{simulation}
\begin{tabular}{lllllll}
 Algorithm & Y & Q & M & W & D &  H \\ \hline
 automated ETS & \checkmark & \checkmark & \checkmark &  &  &  \\
automated ARIMA & \checkmark & \checkmark & \checkmark &  &  &  \\
forecast based on multiple seasonal decomposition &  &  &  & \checkmark & \checkmark & \checkmark\\ \hline
\end{tabular}
\end{table}

As illustrated in \textcite{fforms}, the observed time series and the
simulated time series form the reference to build our classification
algorithm. Once we create the reference set for random forest training
we split each time series in the reference set into training period and
test period.

\subsubsection{Input: features}\label{input-features}

The FFORMS framework operates on the features of the time series. For
each time series in the reference set features are calculated based on
the training period of the time series.

\begin{table}[!htp]
\centering\footnotesize\tabcolsep=0.12cm
\caption{Time series features}
\label{feature}
\begin{tabular}{llp{8,8cm}cccc}
\toprule
\multicolumn{2}{c}{Feature} & Description & Y & Q/M & W & D/H\\
\midrule
1  & T              & length of time series                                                                   & \yes  & \yes & \yes & \yes\\
2  & trend          & strength of trend                                                                       & \yes  & \yes & \yes & \yes\\
3  & seasonality 1    & strength of seasonality corresponds to frequency 1                                                              & -     & \yes & \yes & \yes\\
4  & seasonality 2    & strength of seasonality corresponds to frequency 2                                                              & -     & - & -& \yes\\
5  & linearity      & linearity                                                                               & \yes  & \yes & \yes & \yes\\
6  & curvature      & curvature                                                                               & \yes  & \yes & \yes & \yes\\
7  & spikiness      & spikiness                                                                               & \yes  & \yes & \yes & \yes\\
8  & e\_acf1        & first ACF value of remainder series                                                     & \yes  & \yes & \yes & \yes\\
9  & stability      & stability                                                                               & \yes  & \yes & \yes & \yes\\
10  & lumpiness      & lumpiness                                                                               & \yes  & \yes & \yes & \yes\\
11 & entropy        & spectral entropy                                                                        & \yes  & \yes & \yes & \yes\\
12 & hurst          & Hurst exponent                                                                          & \yes  & \yes & \yes & \yes\\
13 & nonlinearity   & nonlinearity                                                                            & \yes\ & \yes & \yes & \yes\\
14 & alpha          & ETS(A,A,N) $\hat\alpha$                                                                 & \yes  & \yes & \yes & -\\
15 & beta           & ETS(A,A,N) $\hat\beta$                                                                  & \yes  & \yes & \yes & - \\
16 & hwalpha        & ETS(A,A,A) $\hat\alpha$                                                                 & -     & \yes & - & -\\
17 & hwbeta         & ETS(A,A,A) $\hat\beta$                                                                  & -     & \yes & - & - \\
18 & hwgamma        & ETS(A,A,A) $\hat\gamma$                                                                 & -     & \yes & - &-\\
19 & ur\_pp         & test statistic based on Phillips-Perron test                                            & \yes  & - & - & - \\
20 & ur\_kpss       & test statistic based on KPSS test                                                       & \yes  & - & - & - \\
21 & y\_acf1        & first ACF value of the original series                                                  & \yes  & \yes & \yes & \yes\\
22 & diff1y\_acf1   & first ACF value of the differenced series                                               & \yes  & \yes & \yes & \yes\\
23 & diff2y\_acf1   & first ACF value of the twice-differenced series                                         & \yes  & \yes & \yes & \yes\\
24 & y\_acf5        & sum of squares of first 5 ACF values of original series                                 & \yes  & \yes & \yes & \yes\\
25 & diff1y\_acf5   & sum of squares of first 5 ACF values of differenced series                              & \yes  & \yes & \yes & \yes\\
26 & diff2y\_acf5   & sum of squares of first 5 ACF values of twice-differenced series                        & \yes  & \yes & \yes & \yes \\
27 & seas\_acf1     & autocorrelation coefficient at first seasonal lag                                       & -     & \yes & \yes & \yes\\
28 & sediff\_acf1   & first ACF value of seasonally-differenced series                                        & -     & \yes & \yes & \yes\\
29 & sediff\_seacf1 & ACF value at the first seasonal lag of seasonally-differenced series                    & -     & \yes & \yes & \yes\\
30 & sediff\_acf5   & sum of squares of first 5 autocorrelation coefficients of seasonally-differenced series & -     & \yes & \yes & \yes\\
31 & seas\_pacf     & partial autocorrelation coefficient at first seasonal lag & -     & \yes & \yes & \yes\\
32 & lmres\_acf1    & first ACF value of residual series of linear trend model                                & \yes  & - & - & -\\
33 & y\_pacf5       & sum of squares of first 5 PACF values of original series                                & \yes  & \yes & \yes & \yes\\
34 & diff1y\_pacf5  & sum of squares of first 5 PACF values of differenced series                             & \yes  & \yes & \yes & \yes\\
35 & diff2y\_pacf5  & sum of squares of first 5 PACF values of twice-differenced series                       & \yes  & \yes & \yes & \yes\\
\bottomrule
 \end{tabular}
\end{table}

The description of the features calculated under each frequency category
is shown in Table \ref{feature}. A comprehensive description of the
features used in the experiment is given in \textcite{fforms}.

\subsubsection{Output: class-labels}\label{output-class-labels}

In addition to the class labels used by \textcite{fforms} we include
some more class labels when applying the FFORMS framework to the M4
competition time series. The description of class labels considered
under each frequency is shown in Table \ref{classlabels}. We fit the
corresponding models outlined in Table \ref{classlabels} to each series
in the reference set. The models are estimated using the training period
for each series, and forecasts are produced for the test periods.

\begin{table}[!htp]
\centering\footnotesize\tabcolsep=0.12cm
\caption{Class labels}
\label{classlabels}
\begin{tabular}{llrrrr}
class label & Description & Y & Q/M & W & D/H \\ \hline
WN & white noise process & \checkmark & \checkmark & \checkmark & \checkmark \\
AR/MA/ARMA & AR, MA, ARMA processes & \checkmark & \checkmark & \checkmark & -\\
ARIMA & ARIMA process & \checkmark & \checkmark & \checkmark & - \\
SARIMA & seasonal ARIMA & \checkmark & \checkmark & \checkmark & -\\
RWD & random walk with drift & \checkmark & \checkmark & \checkmark & \checkmark \\
RW & random walk & \checkmark & \checkmark & \checkmark & \checkmark  \\
Theta & standard theta method & \checkmark & \checkmark & \checkmark & \checkmark \\
STL-AR &  & - & \checkmark & \checkmark & \checkmark \\
ETS-notrendnoseasonal & ETS without trend and seasonal components & \checkmark & \checkmark & \checkmark & - \\
ETStrendonly & ETS with trend component and without seasonal component & \checkmark & \checkmark & \checkmark & -\\
ETSdampedtrend & ETS with damped trend component and without seasonal component  & \checkmark &  \checkmark & - & - \\
ETStrendseasonal & ETS with trend and seasonal components & - & \checkmark & - & - \\
ETSdampedtrendseasonal & ETS with damped trend and seasonal components & - & \checkmark & - & -\\
ETSseasonalonly & ETS with seasonal components and without trend component & -  & \checkmark & - & - \\
snaive & seasonal naive method & \checkmark & \checkmark & \checkmark & \checkmark \\
tbats & TBATS forecasting & - & \checkmark & \checkmark & \checkmark \\
nn & neural network time series forecasts & \checkmark & \checkmark & \checkmark & \checkmark \\
mstlets &  & - & - & \checkmark & \checkmark \\
mstlarima & & - & - & - & \checkmark \\\hline
\end{tabular}
\end{table}

The \texttt{auto.arima} and \texttt{ets} functions in the forecast
package are used to identify the suitable (S)ARIMA and ETS models. In
order to identify the ``best'' forecast-model for each time series in
the reference set we combine the mean Absolute Scaled Error (MASE) and
the symmetric Mean Absolute Percentage Error (MAPE) calculated over the
test set. More specifically, for each series both forecast error
measures MASE and sMAPE are calculated for each of the forecast models.
Each of these is respectively standardized by the median MASE and median
sMAPE calculated across the methods. The model with the lowest average
value of the scaled MASE and scaled sMAPE is selected as the output
class-label. Most of the labels given in Table \ref{classlabels} are
self-explanatory labels. In STL-AR, mstlets, and mstlarima, first STL
decomposition method applied to the time series and then seasonal naive
method is used to forecast the seasonal component. Finally, AR, ETS and
ARIMA models are used to forecast seasonally adjusted data respectively.

\subsubsection{Train a random forest
classifier}\label{train-a-random-forest-classifier}

A random forest with class priors is used to develop the classifier. We
build separate random forest classifiers for yearly, quarterly, monthly,
weekly, daily and hourly time series. The wrapper function called
\texttt{build\_rf} in the \texttt{seer} package enables the training of
a random forest and returns class labels(``best'' forecast-model) for
each time series.

\subsection{FFORMS framework: online
phase}\label{fforms-framework-online-phase}

The online phase of the algorithm involves generating point forecasts
and 95\% prediction intervals for the M4 competition data. First, the
corresponding features are calculated based on the full length of the
training period provided by the M4 competition. Second, point forecasts
and 95\% prediction intervals are calculated based on the predicted
class labels, in this case forecast-models. Finally, all negative values
are set to zero.

\section{Peeking inside FFORMS}\label{machinelearning}

The main theme of this paper is to explore the nature of the
relationship between features and forecast-model selection. We use both
model-diagnostic approaches and machine learning interpretability
approaches.

\subsection{Model-diagnostics}\label{model-diagnostics}

Model-diagnostic is an important aspect in evaluating the accuracy of
the model's predictions as well as the model's understanding of the
nature of the relationship between features and predicted outcome.

\subsubsection{Out-of-bag(OOB) error and uncertainty measure for each
observation}\label{out-of-bagoob-error-and-uncertainty-measure-for-each-observation}

It is argued in order to estimate the test error of a bagged model it is
not necessary to perform cross-validation approach, because each tree is
grown using different bootstrap samples from the training set and a part
of training data is not used in the tree construction
(\textcite{breiman2001random}; \textcite{chen2004using}). In general,
each bagged tree does not make use of around one third of observations
to construct the decision tree. These observations are referred to as
the out-of-bag(OOB) observations. Each tree is grown based on different
bootstrap samples hence, each tree has different set of OOB
observations. These OOB samples can be used to calculate internal
estimation of the test set error.

\subsubsection{Representation of model in the data space and data in the
model space
(d-in-ms)}\label{representation-of-model-in-the-data-space-and-data-in-the-model-space-d-in-ms}

\textcite{wickham2015visualizing} explains the importance of displaying
the ``model in the data space (m-in-ds)'' and ``data in the model space
(d-in-ms)''. Displaying the data in the model space(d-in-ms) is the most
commonly used approach for model-diagnostics. For example, plot of
fitted values versus residuals (\textcite{wickham2015visualizing}).
D-in-MS is a visualisation of embedding high-dimensional data into a
low-dimensional space generated from the model. Visualisation of D-in-MS
do not help to gain an understanding of the nature of the relationship
between features predicted outcome. In order to address this issue
\textcite{wickham2015visualizing} and \textcite{da2017interactive} have
highlighted the importance of visualisations of model in the data space.
In the context of classification, representation of m-in-ds could be
achieved by first, projecting the training data set into meaningful
low-dimensional feature space and then visualize the complete prediction
regions or their boundaries. In other words this can be considered as
the visualization of predictor space in the context of the data space.
See \textcite{wickham2015visualizing} for visualisation method of this
kind and \textcite{da2017interactive} for comparable method for random
forest algorithms.

\subsection{Machine Learning
Interpretability}\label{machine-learning-interpretability}

In recent years, there have been a growing interest for interpretability
of machine learning algorithms with European General Data Protection
Regulation (GDPR) stipulates the explainability of all automatically
made decision concerning individuals. We explore the role of features in
two different perspectives: i) global explanation of feature
contribution: overall role of features in the choice of different
forecast model selection, and ii) local explanation of feature
contribution: nature of the contributions features make for a prediction
of a specific instance. We will introduce each of these ideas briefly
below. Model-diagnostics tools are used.

\subsection{General Notation}\label{general-notation}

Let \(\mathcal{P}=\{(\mathbf{x^{(i)}}, y^{(i)})\}_{i=1}^{N}\) be the
historical data set we use to train the classifier. Consider a
p-dimensional feature vector \(X=(X_1, X_2, ..., X_p)\) and a dependent
variable, best forecasting method for each series \(Y\). Let
\(\mathcal{G}\) be the unknown relationship between \(X\) and \(Y\).
\textcite{Zhao} term this as ``law of nature''. Inside the FFORMS
framework, random forest algorithm tries to learn this relationship
using the historical data we provided. We denote the predicted function
as \(g\).

\subsection{Global Interpretability
Methods}\label{global-interpretability-methods}

Global interpretability evaluate the behavior of a model on entire data
set. Global perspective of model interpretation helps users to
understand the overall modeled relationship between features and the
model outcome. For example, which features are contribute mostly to the
predictive mechanism of the fitted model, complex interactions between
features, etc. In the following subsections we provide a description of
tools we use to explore the global perspective of the model.

\subsection{Analysis of Feature
contribution}\label{analysis-of-feature-contribution}

\textcite{jiang2002} explains variable importance under three different
views: i) causality: change in the value of Y for a increase or decrease
in the value of x, ii) contribution of X based on out-of-sample
prediction accuracy and iii) face value of X on prediction function
\(g\), for example in linear regression model estimated coefficients of
each predictor can be considered as a measure of variable importance.
See \textcite{jiang2002} for comparable face value interpretation for
machine learning models. In this paper we use the first two notions of
variable importance. Partial dependency functions and individual
conditional expectation curves are used to explore the ``causality''
notion of variable importance while Mean decrease in Gini coefficient
and Permutation-based variable importance are used to capture the second
notion of variable importance-features contribution to the predictive
accuracy(\textcite{Zhao}). We will introduce each of these variable
importance measures below.

\subsubsection{Mean decrease in Gini
coefficient}\label{mean-decrease-in-gini-coefficient}

Mean decrease in Gini coefficient is a measure of how each feature
contributes to the homogeneity of the nodes and leaves in the resulting
random forest proposed by \textcite{breiman2001random}.

\subsubsection{Permutation-based variable importance
measure}\label{permutation-based-variable-importance-measure}

The permutation-based variable importance introduced by
\textcite{breiman2001random} measures the the prediction strength of
each feature. This measure is calculated based on the out-of-bag (OOB)
observations. The calculation of variable importance is formalized as
follow: Let \(\bar{\mathcal{B}}^{(k)}\) be the OOB sample for a tree
\(k\), with \(k\in \{1,...,ntree\}\), where \(ntree\) is the number of
trees in the random forest. Then the variable importance of variable
\(X_{j}\) in \(k^{th}\) tree is:
\[VI^{(k)}(X_{j})=\frac{\sum_{i\in \bar{\mathcal{B}}^{(k)}}I(\gamma_{i}=\gamma_{i,\pi_{j}}^{k})}{|\bar{\mathcal{B}}^{(k)}|}-\frac{\sum_{i\in \bar{\mathcal{B}}^{(k)}}I(\gamma_{i}=\gamma_{i}^{k})}{|\bar{\mathcal{B}}^{(k)}|},\]
where \(\gamma_{i}^{k}\) denotes the predicted class for the \(i^{th}\)
observation before permuting the values of \(X_{j}\) and
\(\gamma_{i, \pi_{j}}^{k}\) is the predicted class for the \(i^{th}\)
observation after permuting the values of \(X_{j}\). The overall
variable importance score is calculated as:
\[VI(X_{j})=\frac{\sum_{1}^{ntree}VI^{(t)}(x_{j})}{ntree}.\]

Permutation-based variable importance measures provide a useful starting
point for identifying relative influence of features on the predicted
outcome. However, they provide a little indication of the nature of the
relationship between the features and model outcome. To gain further
insights into the role of features inside the FFORMS framework we use
partial dependence plot (PDP) introduced by
\textcite{friedman2008predictive}.

\subsubsection{Partial dependence plot
(PDP)}\label{partial-dependence-plot-pdp}

Partial dependence plot can be used to graphically examine how each
feature is related to the model prediction while accounting for the
average effect of other features in the model. Let \(X_s\) be the subset
of feature we want examine partial dependencies for and \(X_c\) be the
remaining set of features in \(X\). Then \(g_s\), the partial dependence
function on \(X_s\) is defines as
\[g_s(X_s)=E_{x_c}[g(x_s, X_c)]=\int{g(x_s, x_c)dP(x_c).}\] In practice,
PDP can be estimated from a training data set as
\[\bar{g_s}(x_s)=\frac{1}{n}\sum_{i=1}^{n}g(x_s, X_{iC}),\] where \(n\)
is the number of observations in the training data set. Partial
dependency curve can be created by plotting the pairs of
\(\{(x_s^k, \bar{g}_s(x_{sk}))\}_{k=1}^{m}\) defined on grid of points
\(\{x_{s1}, x_{s2},\dots, x_{sm}\}\) based on \(X_s\). FFORMS framework
have treated the forecast-model selection problem as a classification
problem. Hence, in this paper partial dependency functions displays the
probability of certain class occurring given different values of feature
\(X_s\).

\subsubsection{Variable importance measure based on
PDP}\label{variable-importance-measure-based-on-pdp}

\textcite{Greenwell2018} introduced a variable importance measure based
on the partial dependency curves. The idea is to measure the
``flatness'' of partial dependence curves for each feature. A feature
whose PDP curve is flat, relative to the other features, indicates that
the feature does not have much influence on the predicted value as it
changes while taking into account the average effect of the other
features in the model. The flatness of the curve is measured using the
standard deviation of the values \(\{\bar{g}_{s}(x_{sk})\}_{k=1}^{m}\).

\subsubsection{Individual Conditional Expectation (ICE)
curves}\label{individual-conditional-expectation-ice-curves}

While partial dependency curves are useful in understanding the
estimated relationship between feature and predicted outcome in the
presence of substantial interaction between features, it can be
misleading. \textcite{goldstein2015peeking} proposed the Individual
Conditional Expectation (ICE) curves to address this issue. Instead of
averaging \(g(x_s, X_{iC})\) over all observations in the training data,
ICE plots the individual response curves by plotting the pairs
\(\{(x_s^k, g(x_{sk}, X_{iC}))\}_{k=1}^{m}\) defined on grid of points
\(\{x_{s1}, x_{s2},\dots, x_{sm}\}\) based on \(X_s\). In other words
partial dependency curve is simply the average of all the ICE curves.

\subsubsection{Variable importance measure based on ICE
curves}\label{variable-importance-measure-based-on-ice-curves}

This method is similar to the PDP-based VI scores above, but are based
on measuring the ``flatness'' of the individual conditional expectation
curves. We calculated standard deviations of each ICE curve. We then
computed a ICE based variable importance score -- simply the average of
all the standard deviations. A higher value indicates a higher degree of
interactivity with other features.

\subsection{Assessment of Interaction
Effect}\label{assessment-of-interaction-effect}

Friedman's H-statistic (\textcite{friedman2008predictive}) is use to
test the presence of interaction between all possible pair of features.
This statistic is computed based on the partial dependence functions.
For two way interaction between two specific variable \(x_j\) and
\(x_k\), Friedman's H-statistic is defined as follow,

\[H_{jk}^2=\sum_{i=1}^{n}[\bar{g}_{s}(x_{ij}, x_{jk})-\bar{g}_{s}(x_{ij})-\bar{g}_{s}(x_{ik})]^2/\sum_{i=1}^{n}\bar{g}^2_{s}(x_{ij}, x_{jk}).\]

The Friedman's H-statistic measures the fraction of variance of
two-variable partial dependency, \(\bar{g}_{s}(x_{ij}, x_{jk})\) not
captured by sum of the respective individual partial dependencies,
\(\bar{g}_{s}(x_{ij})+\bar{g}_{s}(x_{ik})\). In addition to Friedman's
H-statistic we also use the PDP of two variables to visualize the
interaction effect.

Note that the, PD plots, ICE curves and PD-, ICE-associated measures and
Friedman's H-statistic are computationally intensive to compute,
especially when there are large number of observations in the training
set. Hence, in our experiments ICE and PDP-based variable importance are
measured based on the subset of randomly selected training examples.

\subsection{Local Interpretable Model-agnostic Explanations
(LIME)}\label{local-interpretable-model-agnostic-explanations-lime}

Global interpretations help us to understand entire modeled
relationship. Local interpretations help us to understand the
predictions of the model for a single instance or a group of similar
instances. In other words this allows users to zoom into a particular
instance or a subset and explore how different features affect the
resulting prediction. We use Local Interpretable Model-agnostic
Explanations (LIME) approach introduce by \textcite{ribeiro2016should}
for explaining individual predictions which relies on the assumption
that ``every complex model is linear on a local scale''. This is
accomplished by locally approximating the complex black-box model with a
simple interpretable model. \textcite{ribeiro2016should} highlighted
features that are globally important may not be in the local context and
vice verse. The algorithm steps can be summarized as follow:

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  Select an observation of interest which we need to have explanations
  for its black-box prediction.
\item
  Create a permuted data set based on the selected observation. Permuted
  data set is created by making slight modifications to the features of
  selected observations.
\item
  Obtain similarity scores by calculating distance between permuted data
  and selected observation.
\item
  Obtain predicted outcomes for all permuted data using the black-box
  model.
\item
  Select \(m\) number of features best describing the black-box model
  outcome. This can be accomplished by applying feature selection
  algorithms such as ridge regression, lasso, ridge regression, etc.
\item
  Fit a simple linear model to the permuted data based on \(m\) selected
  features and similarity scores in step 3 as weights and complex model
  prediction outcomes in step 4 as response variable.
\item
  Use the estimated coefficients of simple linear model to explain the
  local behaviour corresponds to the selected observation in step 1.
\end{enumerate}

An alternative for explaining local behaviour of complex models is
proposed by \textcite{lundberg2017unified} based on game theory named
``Shapley values''.

\newpage

\section{Results}\label{results}

\subsection{Yearly data}\label{yearly-data}

\textbf{Model diagnostic: FFORMS framework,Yearly series}

\begin{figure}
\centering
\includegraphics{figures/yearlyoob-1.png}
\caption{\label{fig:yearlyoob}Distribution of proportion of times each
yearly time series was assigned to each class in the forest. Each row
represent the predicted class label and colours of boxplots corresponds
to true class label. There are ten rows in the plot corresponds to each
predicted class represented by Y-axis. X-axis denotes the proportion of
times a time series is classified in each class. On each row,
distribution of correctly classified class dominated the top, indicating
a fairly good classification of the model fitted.}
\end{figure}

\autoref{fig:yearlyoob} shows the distribution of proportion of times
each observation (in our case each time series) was assigned to each
class based on OOB sample. Each row represent the \textbf{predicted
class label} and colour of boxplots corresponds to \textbf{true class
label}. The proportion 1 indicates, that the time series was always
predicted to the corresponding class and 0 being never. This is an
alternative way of visualizing the vote-matrix information in the random
forest model. The other way of representing vote matrix involves ternary
plot (\textcite{sutherland2000orca}) and jittered side-by-side dotplot
\autocites{ehrlinger2015ggrandomforests}{da2017interactive}. To over
come the problem of overlapping classes arises due to the scale of the
training data set, number of classes and similar types of classes of
data, boxplot diagrams are used. On each row of \autoref{fig:yearlyoob}
the distribution of proportions corresponds to the time series in which
the predicted class label and true class label are the same dominates
the top indicating a fairly good classification of the model. In
addition to that, in each row the distributions corresponds to the time
series labeled similar to the predicted class label also dominate the
others. For example, within ETS-trend predicted class, the distributions
correspond to the true class labels, ETS-damped trend, ARIMA, were also
assigned with high probability and less values were assigned to
ARMA/AR/MA, White noise process and ETS (ANN)/ ETS(MNN). This confirms
that our FFORMS framework successfully learnt the similarities and
dissimilarities between the classes itself. Irrespective of the
class-labels yearly series have a high chance of selecting random walk
with drift models. One reason for this is as shown in
\textcite{kang2018efficient} yearly series of M1, M3 and M4 are
generally trended. (Results of M3 and M4 competition) This diagram helps
to evaluate the model performance in the data space
(model-in-the-data-space) (\textcite{da2017interactive}).

\textbf{Feature importance and main effects}

Permutation-based variable importance and Gini feature importance
measure are used to evaluate the overall feature importance. Moreover,
most important features for each class is identified based on three
measures: i) permutation-based variable importance, ii) partial
dependence functions based variable importance and iii) ICE-curves based
variable importance measure. The one that shows the highest importance
is ranked 25, the second best is ranked 24, and so on. Finally, for each
category, an average rank for each feature is computed based on the mean
value of all rankings across all the feature importance metrics
considered. The corresponding results are shown in
\autoref{fig:viyearly}. The features, strength of trend and test
statistic of Phillips--Perron(PP) unit root test, linearity, first
autocorrelation coefficient of the differenced series are appear to be
most important features in each class. On the other hand, sum of squares
of first five autocorrelation coefficients of the twice-difference
series and lumpiness show lowest contribution across many classes. The
length of time series (N) is assigned a high importance in
neural-network class compared to others. This could be due to neural
network approach may be beneficial in forecasting time series with long
history of observations. Further, first correlation coefficient of the
twice-differenced series is appear to be most important in ARIMA class
as this category contains the higher order differenced series. Hurst
exponent and entropy appear to be equally important in stationary
classes. Within ETS-damped trend category beta and curvature ranked as
important features.

\begin{figure}
\centering
\includegraphics{figures/viyearly-1.png}
\caption{\label{fig:viyearly}Feature importance plot for yearly series.
Longer bars indicate more important features. Top 5 features are
highlighted in red.}
\end{figure}

\textbf{Partial dependency curves}

Partial dependency, and associated confidence intervals corresponds to
top-three features of each category are plotted to explore the
relationship between features and predicted outcome. Confidence
intervals also facilitate the indication of interaction effects. Top
three features of each category shows a non-linear relationship with
predicted outcome. ETS (AAN, MAN, ANN, MNN), ARIMA, ARMA/AR/MA, white
noise process and neural network classes show a monotonically increasing
or decreasing relationship with trend while theta class has a parabolic
relationship with trend. First correlation of the residual series of
linear model shows a monotonically increasing relationship with ETS(ANN,
MNN) class, whereas the random walk with drift shows an opposite
relationship.

\textbf{Two-way interaction between features}

The relative strength of two-way interaction between features were
determined using formulae developed by
\textcite{friedman2008predictive}, which is implemented in the iml
(\textcite{molnar2018iml},) package in R. \autoref{fig:friedmany} shows
the heat maps of relative strength of all possible pairwise interactions
for each class. The test statistic of Phillip-Perron test, strength of
trend, and linearity show a weak interaction with other features in all
the class. However, except ETS-trend class, trend and ur\_pp shows an
high level of interactivity. These two features are appear to be among
top 5 in all the classes according to the variable importance measures.
Further, narrow confidence bands corresponds to these features in the
partial dependency plots also confirms the less interactivity. In almost
all the cases partial correlation and auto-correlation based features
are heavily interacting. However, the first correlation coefficient of
the difference series do not interact with other features heavily in the
case of ARIMA class. Further, almost all pair of features appear to be
interacting with in neural network category.
\autoref{fig:twowayinteractionyearly1} and
\autoref{fig:twowayinteractionyearly2} communicate more information
about the nature of two-way feature interaction effect.

\newpage

\begin{figure}
\centering
\includegraphics{figures/pdpyearly-1.png}
\caption{\label{fig:pdpyearly}Partial dependence plots for the top ranked
features from variable importance measures. The shading shows the 95\%
confidence intervals. Y-axis denotes the probability of belong to
corresponding class.}
\end{figure}

\newpage

\begin{figure}
\centering
\includegraphics{figures/friedmany-1.png}
\caption{\label{fig:friedmany}Heat maps of relative strength of all possible
pairwise interactions calclated based on Friedman's H-statistic}
\end{figure}

\newpage

\begin{figure}
\centering
\includegraphics{figures/twowayinteractionyearly1-1.png}
\caption{\label{fig:twowayinteractionyearly1}Visualisation of two-way
interactions using partial-dependence plots for yearly data.}
\end{figure}

\newpage

\begin{figure}
\centering
\includegraphics{figures/twowayinteractionyearly2-1.png}
\caption{\label{fig:twowayinteractionyearly2}Visualisation of two-way
interactions using partial-dependence plots for yearly data.}
\end{figure}

\newpage

\textbf{Local Interpretable Model-agnostic Explanations: yearly series}

\begin{figure}[h]

{\centering \includegraphics{figures/yearlylime-1} 

}

\end{figure}

\begin{figure}[h]

{\centering \includegraphics{figures/yearlylime2-1} 

}

\caption{Local interpretable Model-agnostic explanations for six selected yearly time series. Features denoted with green colour are supporting features for an outcome label and length of the bar is proportional to the weight of a feature.}\label{fig:yearlylime2}
\end{figure}

\autoref{fig:yearlylime2} shows the feature contribution for the
instances highlighted in \autoref{fig:yearlylime}. The low linearity,
with a trend value of less than 0.616, low values for Phillip-Perron
test-statistic, Hurst exponent, autocorrelation coefficients of the time
series and spikiness of the series causes the FFORMS framework to
classify the first series with ARMA/AR/MA. On the other hand, LIME
indicated the low value of first autocorrelation coefficients of the
original series and the residual series of linear regression model
contradicts to this decision. The reason might be these properties of
autocorrelation coefficients native to the random walk models not to the
stationary ARMA/AR/MA models. It is interesting to note that even though
2, 5 and 6 series located close proximity to each other in the PCA space
their, varying degree of trend, spikiness, first autocorrelation
coefficient of the difference series, entropy, led them to classify as
ETS-trend, neural-network and random walk respectively.

\subsection{Quarterly data}\label{quarterly-data}

\autoref{fig:oobquarterly} shows proportion of times each series was
classified to each class by true class labels. For all ETS models the
proportion of times the true class label was identified was higher than
other categories. Within snaive, stlar, tbats and white noise classes
ETS-seasonal, SARIMA, ETS-trend and ARMA/AR/MA slightly dominates
respectively. It could be due to the high-similarity to one another.
Further, withing the neural network category ETS (ANN/MNN), ARMA/AR/MA
and white noise processes dominates equally. This indicates neural
network models share similar type of features with those three classes.
Furthermore, except the time series labeled as a ETS models with
seasonal component or SARIMA models all other time series have a high
probability of being in random walk with drift class irrespective of
their true class labels. These results are consisted with the random
walk model fitted to yearly data. Except the time series labeled as
ARMA/AR/MA all other quarterly time series have a very low chance of
being to ARMA/AR/MA class. Further, all distributions corresponds to the
tbats row located further away from zero. This indicates all time series
select tbats model at least once from the individual trees in the
forest. Except few outliars, distributions within neural network
category also show a slight upward deviation from zero. However, the
upper boundary of these distributions do not surpass the upper
boundaries of dominating box plots in the random walk with drift class
and SARIMA class. These types of diversity in the distributions
indicates the the appropriateness of using combination forecasting.
Further, these information are useful in identifying potential time
series models for combination forecast and improve the existing
combination approaches proposed in the M4-competition.

\textbf{Variable importance: quarterly data}

Strength of seasonality appear to be the most important feature across
all categories. Similar to the results of yearly time series trend and
linearity also listed among the top five features in each category. In
the case of yearly data low variable importance is assigned to both
stability and length of the series, however with quarterly time series
within most categories stability and length of the series are assigned a
high importance. In addition to the strength of seasonality, models with
seasonal components assigned high importance to additional features
related to seasonality such as ACF or PACF feature related to seasonal
lag or seasonally-differenced series, for example, snaive and
ETS-seasonal with partial autocorrelation coefficient at first seasonal
lag, etc. Furthermore, as expected features related to parameter
estimates ETS (A, A, A) were selected as important features by ETS with
damped trend and seasonal component and ETS with trend and seasonal
component models.

\textbf{Partial dependency curves: quarterly data}

\autoref{fig:pdpquarterly1} shows the partial dependency functions for
top three features with in each category. Seasonality shows a linear
relationship with probability of being to snaive class. Random walk,
random walk with drift, all ETS models without seasonal component, ARIMA
and white noise process show a similar pattern of relationship with
seasonality with vary degree of width of confidence band while all ETS
models with seasonal components and SARIMA hold the opposite patter.
SARIMA and ARIMA classes shows a non-monotonic relationship with
seasonality. Neural network class shows a non-linear relationship with
seasonality, and wide confidence bands indicated seasonality interact
with other features when deciding probability of being into neural
network class. According to the partial dependency functions corresponds
to the rows ETS-NTNS, ETS-DT, ETS-T and ETS-TS confirms how well the
model's understanding of feature behaviour matches the domain's expert
knowledge which gives rise to the trustability of the FFORMS framework.
For example, highly trended and low seasonality time series have a high
chance of being classified to ETS-Trend class while the opposite
relationship can be seen in ETS-seasonal category with low trended and
highly seasonally oscillated time series have a high chance of being
classified to ETS-seasonal. Even though, two different portfolio of time
series are used to train two different classifiers for yearly and
quarterly data partial dependency functions show a consistency between
the results of yearly framework and quarterly framework. For example,
partial dependency function of linearity within ARMA/AR/MA class,
partial dependency functions corresponds to trend, etc.
\autoref{fig:friedmanQ} shows heat maps of relative strength of all
possible pairwise interactions calculated based on Friedman's
H-statistic. Within all classes strength of seasonality shows less
interactivity with other features. Except ETS-damped trend, SARIMA and
ARIMA classes trend interact highly with other features. Further,
Friedman's H statistic reflect high interactivity between
autocorrelation and partial autocorrelation in SARIMA, ARIMA and
ARMA/AR/MA classes. The features alpha and curvature shows very high
interactivity with other features within ETS-damped trend, ETS-trend and
neural network classes. Length of the time series show very low
interactivity with others within snaive class.

\newpage

\begin{figure}
\centering
\includegraphics{figures/oobquarterly-1.png}
\caption{\label{fig:oobquarterly}Distribution of proportion of times each
quarterly time series was assigned to each class in the forest. Each row
represent the predicted class label and colour of boxplots corresponds
to true class label. X-axis denotes the proportion of times a time
series is classified in each class.}
\end{figure}

\newpage

\begin{figure}[h]

{\centering \includegraphics{figures/viquarterly-1} 

}

\caption{Feature importance plot for quarterly series. Longer bars indicate more important features. Top 5 features are highlighted in red.}\label{fig:viquarterly}
\end{figure}

\newpage

\begin{figure}
\centering
\includegraphics{figures/pdpquarterly1-1.png}
\caption{\label{fig:pdpquarterly1}Partial dependence plots for top three
features in each category (Quarterly)}
\end{figure}

\newpage

\begin{figure}
\centering
\includegraphics{figures/pdpquarterly2-1.png}
\caption{\label{fig:pdpquarterly2}Partial dependence plots for top three
features in each category (Quarterly)}
\end{figure}

\newpage

\begin{figure}
\centering
\includegraphics{figures/friedmanQ-1.png}
\caption{\label{fig:friedmanQ}Heat maps of relative strength of all possible
pairwise interactions calclated based on Friedman's H-statistic for
quarterly data.}
\end{figure}

\newpage

\begin{figure}
\centering
\includegraphics{figures/friedmanQ2-1.png}
\caption{\label{fig:friedmanQ2}Heat maps of relative strength of all
possible pairwise interactions calclated based on Friedman's H-statistic
for quarterly data(cont).}
\end{figure}

\newpage

\textbf{Local Interpretable Model-agnostic Explanations: quarterly
series}

\begin{figure}[h]

{\centering \includegraphics{figures/quarterlylime-1} 

}

\end{figure}

\begin{figure}[h]

{\centering \includegraphics{figures/quarterlylime2-1} 

}

\caption{Local interpretable Model-agnostic explanations for four selected quarterly time series. Features denoted with green colour are supporting features for an outcome label and length of the bar is proportional to the weight of a feature.}\label{fig:quarterlylime2}
\end{figure}

\subsection{Monthly}\label{monthly}

\autoref{fig:oobmonthly} can be interpreted similar to the results of
monthly data. For quarterly and monthly data same set of features and
class-labels are used to training the model. Hence, this consistency
between the results of quarterly and monthly series would provide
evidence in support of the validity and trustability of the model.
Seasonality, trend, spikiness and linearity appear to be most important
features across all categories. Further, features calculated based on
parameter estimates of ETS(A,A,A), ACF and PACF-based features related
to seasonal lags and seasonally differenced series were assigned higher
importance than the results of quarterly series. One notable difference
between quarterly series and monthly series is, length of the series was
assigned a very high importance specially in random walk with drift,
random walk, ETS with out seasonal and trend components, ETS with
seasonal components, SARIMA and ARMA/AR/MA classes. Partial dependence
plots of the top 3 features of each class is represented in
\autoref{fig:pdpmonthly1} and \autoref{fig:pdpmonthly2}. Probability of
selecting a model with a parameter to capture the seasonal effect
(snaive, all ETS models with seasonal component, SARIMA, tbats, theta,
stlar) increases as the seasonality increases. Probability of selecting
a neural network approach decreases as seasonality increases but this
probability increases as stability increases. PPD plots also reveals
FFORMS framework select ETS-trend models for short, less seasonally and
highly trended (strength of trend close to 1) time series while random
walk with drift and random walk series are suitable for short, less
seasonally series with trend vary between 0.75 to 0.85. According to
\autoref{fig:friedmanM} and \autoref{fig:friedmanM2} within random walk
and ARMA/AR/MA classes trend, seasonality, stability and length of the
time series show less interactivity with other features. Trend shows
high interactivity with other features within random walk, all classes
with ETS model with damped trend component and theta . In general,
spikiness, also shows interactivity with other features within many
classes.

\clearpage

\begin{figure}
\centering
\includegraphics{figures/oobmonthly-1.png}
\caption{\label{fig:oobmonthly}Distribution of proportion of times each
monthly time series was assigned to each class in the forest. Each row
represent the predicted class label and colour of boxplots corresponds
to true class label. X-axis denotes the proportion of times a time
series is classified in each class.}
\end{figure}

\newpage

\begin{figure}[h]

{\centering \includegraphics{figures/vimonthly-1} 

}

\caption{Feature importance plot for monthly series. Longer bars indicate more important features. Top 5 features are highlighted in red.}\label{fig:vimonthly}
\end{figure}

\clearpage

\begin{figure}
\centering
\includegraphics{figures/pdpmonthly1-1.png}
\caption{\label{fig:pdpmonthly1}Partial dependence plots for top three
features in each category (Monthly)}
\end{figure}

\newpage

\begin{figure}
\centering
\includegraphics{figures/pdpmonthly2-1.png}
\caption{\label{fig:pdpmonthly2}Partial dependence plots for top three
features in each category (Monthly)}
\end{figure}

\newpage

\begin{figure}
\centering
\includegraphics{figures/friedmanM-1.png}
\caption{\label{fig:friedmanM}Heat maps of relative strength of all possible
pairwise interactions calclated based on Friedman's H-statistic for
monthly data.}
\end{figure}

\newpage

\begin{figure}
\centering
\includegraphics{figures/friedmanM2-1.png}
\caption{\label{fig:friedmanM2}Heat maps of relative strength of all
possible pairwise interactions calclated based on Friedman's H-statistic
for monthly data.}
\end{figure}

\subsection{Weekly}\label{weekly}

\autoref{fig:oobweekly} proportion of times each time series was
classified to each class. Unlike, yearly, quarterly and monthly data
theta models have low chance of being selected to forecast weekly data.
The random walk with drift, tbat models and theta have higher chance of
being selected. Except ARMA/AR/MA class the distribution corresponds to
the true class label dominates others. ARMA/AR/MA class shows some
unusual behaviour within some categories due to class imbalance ratio,
ARMA/AR/MA class contains fewer number of observations in the training
set. According to the results of \autoref{fig:viweekly} spikiness,
linearity, trend, strength of seasonality, stability and lumpiness has
been assigned a high importance. This is similar to the results of
yearly, quarterly and monthly data. The length of series has been
selected among top 5 by mstlets, tbats, theta and neural network models.
According to the results of \autoref{fig:weeklypdp} for mstlets models
probability of being selected increases as the length of series
increases while the opposite relationship was observed at neural network
models. The tabts models show a non-monotonic relationship with the
length of series. It is surprising to observe that for mstlets models
probability of being selected decreases an seasonality increases. This
could be due to the interaction effect of seasonality with others. For
weekly data, the number of pairs showing a particular degree of
interaction strength is relatively low compared to other yearly,
quarterly and monthly data. In tbats class, trend, linearity, spikiness,
seasonality and first autocorrelation coefficient of the seasonally
differenced series interact heavily with other features.

\begin{figure}
\centering
\includegraphics{figures/oobweekly-1.png}
\caption{\label{fig:oobweekly}Distribution of proportion of times each
weekly time series was classified to each class in the forest. Each row
represent the predicted class label and colour of boxplots corresponds
to true class label. There are ten rows in the plot corresponds to each
predicted class represented by Y-axis. X-axis denotes the proportion of
times a time series is classified in each class.}
\end{figure}

\clearpage

\begin{figure}[h]

{\centering \includegraphics{figures/viweekly-1} 

}

\caption{Feature importance plot for weekly series. Longer bars indicate more important features. Top 5 features are highlighted in red.}\label{fig:viweekly}
\end{figure}

\begin{figure}
\centering
\includegraphics{figures/weeklypdp-1.png}
\caption{\label{fig:weeklypdp}Partial dependence plots for the top ranked
features from variable importance measures(weekly series). The shading
shows the 95\% confidence intervals. Y-axis denotes the probability of
belong to corresponding class.}
\end{figure}

\newpage

\begin{figure}
\centering
\includegraphics{figures/weeklypdp2-1.png}
\caption{\label{fig:weeklypdp2}Partial dependence plots for the top ranked
features from variable importance measures(weekly series). The shading
shows the 95\% confidence intervals. Y-axis denotes the probability of
belong to corresponding class(cont.).}
\end{figure}

\newpage

\begin{figure}
\centering
\includegraphics{figures/friedmanHW-1.png}
\caption{\label{fig:friedmanHW}Heat maps of relative strength of all
possible pairwise interactions calclated based on Friedman's H-statistic
for weekly data.}
\end{figure}

\newpage

\subsection{Daily}\label{daily}

According to \autoref{fig:oobdaily} the distributions corresponds to
observations that have been correctly classified dominate the top,
indicating a fairly good classification from FFORMS framework. However,
there are few observations that have been incorrectly classified to
tbats class with very high probabilities. In general, neural network
models have higher chance of being selected for daily time series.

\begin{figure}
\centering
\includegraphics{figures/oobdaily-1.png}
\caption{\label{fig:oobdaily}Distribution of proportion of times each daily
time series was assigned to each class in the forest. Each row represent
the predicted class label and colour of boxplots corresponds to true
class label. There are ten rows in the plot corresponds to each
predicted class represented by Y-axis. X-axis denotes the proportion of
times a time series is classified in each class. On each row,
distribution of correctly classified class dominated the top, indicating
a fairly good classification of the model fitted.}
\end{figure}

\newpage

\begin{figure}[h]

{\centering \includegraphics{figures/vidaily-1} 

}

\caption{Feature importance plot for daily series. Longer bars indicate more important features. Top 5 features are highlighted in red.}\label{fig:vidaily}
\end{figure}

Variable importance graph for daily data is shown in
\autoref{fig:vidaily}. Overall, similar to the previous results most
important features for determining suitable forecast-models for daily
time series are, strength of seasonality corresponds to the weekly
seasonality (7), stability, trend, lumpiness and linearity. Further
more, length of the series is important in determining random walk,
random walk with drift, mstlarima, mstlets, stlar, theta and nn classes.
Strength of seasonality corresponds to the annual seasonality (365.25)
is appeared to be important in determining the selection of theta
models. \autoref{fig:dailypdp} shows the partial dependency plots of the
top 3 features from the FFORMS framework. According to the results of
\autoref{fig:dailypdp} shorter series tends to select random walk with
drift models while probability of selecting tbats, and theta models
increases as the length of series increases. Neural network models shows
a non-monotonic relationship with length of the series (N). The theta
models tends to be selected for series with high annual seasonality but
very low weekly seasonality.

\newpage

\begin{figure}
\centering
\includegraphics{figures/dailypdp-1.png}
\caption{\label{fig:dailypdp}Partial dependence plots for the top ranked
features from variable importance measures(daily series). The shading
shows the 95\% confidence intervals. Y-axis denotes the probability of
belong to corresponding class.}
\end{figure}

\newpage

\begin{figure}
\centering
\includegraphics{figures/friedmandaily-1.png}
\caption{\label{fig:friedmandaily}Heat maps of relative strength of all
possible pairwise interactions calclated based on Friedman's H-statistic
for daily data.}
\end{figure}

\subsection{Hourly}\label{hourly}

The proportion of times each time series was classified to each class
for hourly series are shown in \autoref{fig:hourlyoob}. Except neural
network class, the distribution corresponds to the correct
classification dominated the top. It is important to note that, in
neural network category, distribution corresponds to snaive class
slightly dominated the distribution corresponds to neural network
category. Overall, for hourly series random walk with drift models,
tbats and neural network models have high chance of being selected.
Furthermore, all distributions corresponds to the neural network
category located further away from zero with respect to x-axis
indicates, every time series selects neural network models at least once
from the trees in the random forest. In general, strength of daily
seasonality (period=24) appear to be more important than than the
strength of weekly seasonality (period=168). Furthermore, entropy,
linearity, sum of squares of first 5 coefficients of PACF, curvature,
trend, spikiness and stability were found to be the most important
features in determining best forecasting method for hourly time series.
Although the length of the series found to be one of the most important
feature from previous results, only snaive category ranked it among top
5 for hourly time series. The strength of seasonality corresponds to
weekly seasonality also seems to be one of the most important feature
for the classes snaive, random walk, mstlarima, and tbats. The partial
dependency plots shows that probability of selecting random walk, random
walk with drift, theta model and white noise process decreased with
higher strength of daily seasonality, while the opposite relationship
hold for other classes.On the other hand, probability of selecting
random walk model increased with the increase in strength of weekly
seasonality.

\begin{figure}
\centering
\includegraphics{figures/hourlyoob-1.png}
\caption{\label{fig:hourlyoob}Distribution of proportion of times each
hourly time series was classified to each class in the forest. Each row
represent the predicted class label and colour of boxplots corresponds
to true class label. There are ten rows in the plot corresponds to each
predicted class represented by Y-axis. X-axis denotes the proportion of
times a time series is classified in each class. On each row, the
distribution of correct classification dominated the top.}
\end{figure}

\newpage

\newpage

\begin{figure}
\centering
\includegraphics{figures/hourlypdp-1.png}
\caption{\label{fig:hourlypdp}Partial dependence plots for the top ranked
features from variable importance measures(hourly series). The shading
shows the 95\% confidence intervals. Y-axis denotes the probability of
belong to corresponding class.}
\end{figure}

\newpage

\begin{figure}
\centering
\includegraphics{figures/friedmanhourly-1.png}
\caption{\label{fig:friedmanhourly}Heat maps of relative strength of all
possible pairwise interactions calclated based on Friedman's H-statistic
for hourly data.}
\end{figure}

\section{Discussion and Conclusions}\label{conclusions}

\newpage

\section*{Appendix}\label{appendix}
\addcontentsline{toc}{section}{Appendix}

\subsection*{PCA space}\label{pca-space}
\addcontentsline{toc}{subsection}{PCA space}

\begin{figure}[h]

{\centering \includegraphics{figures/yearlypca-1} 

}

\caption{Distribution of prediction space across instance space for yearly time series. PC1 and PC2 are the first two principal components.}\label{fig:yearlypca}
\end{figure}

\clearpage

\begin{figure}
\centering
\includegraphics{figures/quarterlypca-1.png}
\caption{\label{fig:quarterlypca}Distribution of prediction space across
instance space for quarterly time series. PC1 and PC2 are the first two
principal components.}
\end{figure}

\newpage

\begin{figure}
\centering
\includegraphics{figures/monthlypca-1.png}
\caption{\label{fig:monthlypca}Distribution of prediction space across
instance space for monthly time series. PC1 and PC2 are the first two
principal components.}
\end{figure}

\clearpage

\begin{figure}[h]

{\centering \includegraphics{figures/pcaweekly-1} 

}

\caption{Distribution of prediction space across instance space for weekly time series. PC1 and PC2 are the first two principal components.}\label{fig:pcaweekly}
\end{figure}

\clearpage

\begin{figure}[h]

{\centering \includegraphics{figures/pcadaily-1} 

}

\caption{Distribution of prediction space across instance space for daily time series. PC1 and PC2 are the first two principal components.}\label{fig:pcadaily}
\end{figure}

\begin{figure}[h]

{\centering \includegraphics{figures/pcahourly-1} 

}

\caption{Distribution of prediction space across instance space for hourly time series. PC1 and PC2 are the first two principal components.}\label{fig:pcahourly}
\end{figure}

\clearpage

\printbibliography[title=References]

\end{document}
